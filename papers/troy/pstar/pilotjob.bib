%% This BibTeX bibliography file was created using BibDesk.
%% http://bibdesk.sourceforge.net/
@inproceedings{Lampson:1983:HCS:800217.806614,
 author = {Lampson, Butler W.},
 title = {Hints for computer system design},
 booktitle = {Proceedings of the ninth ACM symposium on Operating systems principles},
 series = {SOSP '83},
 year = {1983},
 isbn = {0-89791-115-6},
 location = {Bretton Woods, New Hampshire, United States},
 pages = {33--48},
 numpages = {16},
 url = {http://doi.acm.org/10.1145/800217.806614},
 doi = {10.1145/800217.806614},
 acmid = {806614},
 publisher = {ACM},
 address = {New York, NY, USA},
} 
[download]
@article{Lampson:1983:HCS:773379.806614,
 author = {Lampson, Butler W.},
 title = {Hints for computer system design},
 journal = {SIGOPS Oper. Syst. Rev.},
 issue_date = {October 1983},
 volume = {17},
 number = {5},
 month = oct,
 year = {1983},
 issn = {0163-5980},
 pages = {33--48},
 numpages = {16},
 url = {http://doi.acm.org/10.1145/773379.806614},
 doi = {10.1145/773379.806614},
 acmid = {806614},
 publisher = {ACM},
 address = {New York, NY, USA},
}

@inproceedings{5171374,
	Author = {Sfiligoi, I. and Bradley, D.C. and Holzman, B. and Mhashilkar, P. and Padhi, S. and Wurthwein, F.},
	Booktitle = {Computer Science and Information Engineering, 2009 WRI World Congress on},
	Date-Added = {2012-07-14 09:11:44 +0000},
	Date-Modified = {2012-07-14 09:11:44 +0000},
	Doi = {10.1109/CSIE.2009.950},
	Keywords = {glideinWMS;grid computing;grid middleware;grid resources;job scheduling complexity;open science grid;grid computing;middleware;scheduling;},
	Month = {31 2009-april 2},
	Pages = {428 -432},
	Title = {The Pilot Way to Grid Resources Using glideinWMS},
	Volume = {2},
	Year = {2009},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/CSIE.2009.950}}

@misc{glideinwms,
	Date-Added = {2012-07-14 09:04:56 +0000},
	Date-Modified = {2012-07-14 09:08:36 +0000},
	Howpublished = {\url{http://www.uscms.org/SoftwareComputing/Grid/WMS/glideinWMS/doc.prd/index.html}},
	Title = {GlideinWMS - The Glidein-based Workflow Management System},
	Year = {2012}}


@inproceedings{pstar-2012,
	Author = {Andre Luckow and Mark Santcroos and Ole Weider and Andre Merzky and Sharath Maddineni and Shantenu Jha},
	Booktitle = {Proceedings of The International ACM Symposium on High-Performance Parallel and Distributed Computing},
	Date-Added = {2011-09-04 16:13:49 +0000},
	Date-Modified = {2011-09-04 16:14:42 +0000},
	Title = {Towards a Common Model for Pilot-Jobs},
	Year = {2012}}
	


@article{1742-6596-78-1-012057,
	Abstract = {The Open Science Grid (OSG) provides a distributed facility where the Consortium members provide guaranteed and opportunistic access to shared computing and storage resources. OSG provides support for and evolution of the infrastructure through activities that cover operations, security, software, troubleshooting, addition of new capabilities, and support for existing and engagement with new communities. The OSG SciDAC-2 project provides specific activities to manage and evolve the distributed infrastructure and support it's use. The innovative aspects of the project are the maintenance and performance of a collaborative (shared & common) petascale national facility over tens of autonomous computing sites, for many hundreds of users, transferring terabytes of data a day, executing tens of thousands of jobs a day, and providing robust and usable resources for scientific groups of all types and sizes. More information can be found at the OSG web site: www.opensciencegrid.org.},
	Author = {Ruth Pordes {\it et al}},
	Journal = {Journal of Physics: Conference Series},
	Number = {1},
	Pages = {012057},
	Title = {The open science grid},
	Url = {http://stacks.iop.org/1742-6596/78/i=1/a=012057},
	Volume = {78},
	Year = {2007},
	Bdsk-Url-1 = {http://stacks.iop.org/1742-6596/78/i=1/a=012057}}

@article{1742-6596-78-1-012057-long,
	Abstract = {The Open Science Grid (OSG) provides a distributed facility where the Consortium members provide guaranteed and opportunistic access to shared computing and storage resources. OSG provides support for and evolution of the infrastructure through activities that cover operations, security, software, troubleshooting, addition of new capabilities, and support for existing and engagement with new communities. The OSG SciDAC-2 project provides specific activities to manage and evolve the distributed infrastructure and support it's use. The innovative aspects of the project are the maintenance and performance of a collaborative (shared & common) petascale national facility over tens of autonomous computing sites, for many hundreds of users, transferring terabytes of data a day, executing tens of thousands of jobs a day, and providing robust and usable resources for scientific groups of all types and sizes. More information can be found at the OSG web site: www.opensciencegrid.org.},
	Author = {Ruth Pordes and Don Petravick and Bill Kramer and Doug Olson and Miron Livny and Alain Roy and Paul Avery and Kent Blackburn and Torre Wenaus and Frank W{\"u}rthwein and Ian Foster and Rob Gardner and Mike Wilde and Alan Blatecky and John McGee and Rob Quick},
	Journal = {Journal of Physics: Conference Series},
	Number = {1},
	Pages = {012057},
	Title = {The open science grid},
	Url = {http://stacks.iop.org/1742-6596/78/i=1/a=012057},
	Volume = {78},
	Year = {2007},
	Bdsk-Url-1 = {http://stacks.iop.org/1742-6596/78/i=1/a=012057}}

@manual{OMG-CORBA303:2004,
	Key = {OMG},
	Keywords = {2004, corba, omg},
	Month = {M},
	Organization = {Object Management Group},
	Posted-At = {2006-10-04 15:54:44},
	Priority = {0},
	Title = {{Common Object Request Broker Architecture: Core Specification}},
	Year = {2004}}

@article{1742-6596-219-6-062049,
	Abstract = {DIRAC, the LHCb community Grid solution, has pioneered the use of pilot jobs in the Grid. Pilot Jobs provide a homogeneous interface to an heterogeneous set of computing resources. At the same time, Pilot Jobs allow to delay the scheduling decision to the last moment, thus taking into account the precise running conditions at the resource and last moment requests to the system. The DIRAC Workload Management System provides one single scheduling mechanism for jobs with very different profiles. To achieve an overall optimisation, it organizes pending jobs in task queues, both for individual users and production activities. Task queues are created with jobs having similar requirements. Following the VO policy a priority is assigned to each task queue. Pilot submission and subsequent job matching are based on these priorities following a statistical approach.},
	Author = {Adrian Casajus and Ricardo Graciani and Stuart Paterson and Andrei Tsaregorodtsev and the Lhcb Dirac Team},
	Journal = {Journal of Physics: Conference Series},
	Number = {6},
	Pages = {062049},
	Title = {DIRAC pilot framework and the DIRAC Workload Management System},
	Url = {http://stacks.iop.org/1742-6596/219/i=6/a=062049},
	Volume = {219},
	Year = {2010},
	Bdsk-Url-1 = {http://stacks.iop.org/1742-6596/219/i=6/a=062049}}

@inproceedings{dare-tg11-gateways,
	Author = {Joohyun Kim and Sharath Maddineni and Shantenu Jha},
	Booktitle = {Proceedings of TeraGrid'11 Extreme Discovery},
	Title = {Building Gateways for Life-Science Applications using the Distributed Adaptive Runtime Environment (DARE) Framework},
	Year = 2011}

@phdthesis{diane-thesis,
	Author = {Jakub Tomasz Moscicki},
	Date-Added = {2011-04-22 19:11:25 +0200},
	Date-Modified = {2011-04-22 19:15:12 +0200},
	School = {University of Amsterdam},
	Title = {Understanding and Mastering Dynamics in Computing Grids: Processing Moldable Tasks with User-Level Overlay},
	Year = {2011}}

@inproceedings{Doraimani:2008:FGS:1383422.1383429,
	Acmid = {1383429},
	Address = {New York, NY, USA},
	Author = {Doraimani, Shyamala and Iamnitchi, Adriana},
	Booktitle = {Proceedings of the 17th international symposium on High performance distributed computing},
	Doi = {http://doi.acm.org/10.1145/1383422.1383429},
	Isbn = {978-1-59593-997-5},
	Keywords = {caching, data management, file grouping, job scheduling, science grids, trace analysis},
	Location = {Boston, MA, USA},
	Numpages = {12},
	Pages = {153--164},
	Publisher = {ACM},
	Series = {HPDC '08},
	Title = {File grouping for scientific data management: lessons from experimenting with real traces},
	Url = {http://doi.acm.org/10.1145/1383422.1383429},
	Year = {2008},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/1383422.1383429}}

@inproceedings{ramakrishnan2011,
	Author = {Zacharia Fadika and Elif Dede and Madhusudhan Govindaraju and Lavanya Ramakrishnan},
	Booktitle = {submitted to The International Conference for High Performance Computing, Networking, Storage, and Analysis},
	Title = {Benchmarking MapReduce Implementations for Application Usage Scenarios},
	Year = {2011}}

@inproceedings{weissman2011,
	Author = {Michael Cardosa and Chenyu Wang and Anshuman Nangia and Abhishek Chandra and Jon Weissman},
	Title = {Exploring MapReduce Efficiency with Highly-Distributed Data},
	Year = {2011}}

@inproceedings{jha2011,
	Author = {Shantenu Jha and J.D. Blower and Neil Chue-Hong and Simon Dobson and Daniel S. Katz and Omer Rana},
	Title = {3DPAS: Distributed Dynamic Data-intensive Programming Abstractions and Systems},
	Year = {2011}}

@inproceeding{gray2000,
	Author = {Jim Gray, Prashant Shenoy},
	Howpublished = {\url{http://research.microsoft.com/en-us/um/people/gray/papers/ms_tr_99_100_rules_of_thumb_in_data_engineering.pdf}},
	Title = {Rules of Thumb in Data Engineering},
	Year = {2000}}

@book{hey2009,
	Abstract = {Increasingly, scientific breakthroughs will be powered by advanced computing computingapabilities that help researchers manipulate and explore massive datasets.

The speed at which any given scientific discipline advances will depend on how well its researchers collaborate with one another, and with technologists, in areas of eScience such as databases, workflow management, visualization, and cloud computing technologies.

In The Fourth Paradigm: Data-Intensive Scientific Discovery, the collection of essays expands on the vision of pioneering computer scientist Jim Gray for a new, fourth paradigm of discovery based on data-intensive science and offers insights into how it can be fully realized.},
	Added-At = {2010-02-16T09:54:37.000+0100},
	Address = {Redmond, Washington},
	Biburl = {http://www.bibsonomy.org/bibtex/28b203c0313656b6ced70c14c86a4c42a/acka47},
	Date-Added = {2011-09-04 20:56:39 +0000},
	Date-Modified = {2011-09-04 20:57:08 +0000},
	Editor = {Tony Hey and Stewart Tansley and Kristin Tolle},
	Interhash = {296450016ca8a5f8ab16ae4d92d1fc15},
	Intrahash = {8b203c0313656b6ced70c14c86a4c42a},
	Keywords = {research scholarly_communication science},
	Publisher = {Microsoft Research},
	Timestamp = {2010-02-16T09:54:37.000+0100},
	Title = {The Fourth Paradigm: Data-Intensive Scientific Discovery},
	Url = {http://research.microsoft.com/en-us/collaboration/fourthparadigm/},
	Year = 2009,
	Bdsk-Url-1 = {http://research.microsoft.com/en-us/collaboration/fourthparadigm/}}

@article{10.1109/MIC.2011.64,
	Address = {Los Alamitos, CA, USA},
	Author = {Ian Foster},
	Doi = {http://doi.ieeecomputersociety.org/10.1109/MIC.2011.64},
	Issn = {1089-7801},
	Journal = {IEEE Internet Computing},
	Pages = {70-73},
	Publisher = {IEEE Computer Society},
	Title = {Globus Online: Accelerating and Democratizing Science through Cloud-Based Services},
	Volume = {15},
	Year = {2011},
	Bdsk-Url-1 = {http://doi.ieeecomputersociety.org/10.1109/MIC.2011.64}}

@misc{webhdfs,
	Howpublished = {\url{http://hadoop.apache.org/common/docs/r1.0.0/webhdfs.html}},
	Key = {webhdfs},
	Title = {{WebHDFS REST API}},
	Year = 2012}

@inproceedings{Moscicki:908910,
	Abstract = { Distributed analysis environment (DIANE) is the result of R D in CERN IT Division focused on interfacing semi-interactive parallel applications with distributed GRID technology. DIANE provides a master-worker workflow management layer above low-level GRID services. DIANE is application and language-neutral. Component-container architecture and component adapters provide flexibility necessary to fulfill the diverse requirements of distributed applications. Physical transport layer assures interoperability with existing middleware frameworks based on Web services. Several distributed simulations based on Geant 4 were deployed and tested in real-life scenarios with DIANE.},
	Author = {Moscicki, J.T.},
	Booktitle = {Nuclear Science Symposium Conference Record, 2003 IEEE},
	Doi = {10.1109/NSSMIC.2003.1352187},
	Issn = {1082-3654},
	Keywords = {CERN IT Division; GRID-enabled physics data simulation; Geant 4; Web services; component adapters; component-container architecture; distributed analysis environment; interoperability; master-worker workflow management layer; middleware frameworks; physical transport layer; physics data analysis; semi-interactive parallel applications; Internet; data analysis; grid computing; middleware; physics computing; workflow management software;},
	Pages = {1617 - 1620},
	Title = {DIANE - distributed analysis environment for GRID-enabled simulation and analysis of physics data},
	Volume = {3},
	Year = {2003},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/NSSMIC.2003.1352187}}

@misc{bigjob_web,
	Howpublished = {\url{http://saga-project.github.com/BigJob/}},
	Key = {SAGA BigJob},
	Title = {{SAGA BigJob}},
	Year = {2012}}

@misc{pilot_api,
	Author = {{Pilot API}},
	Howpublished = {\url{http://saga-project.github.com/BigJob/apidoc/}},
	Key = {Pilot},
	Year = 2012}

@article{condor-g,
	Author = {Frey, J. and Tannenbaum, T. and Livny, M. and Foster, I. and Tuecke, S.},
	Citeulike-Article-Id = {291860},
	Date-Added = {2008-02-28 10:08:47 -0600},
	Date-Modified = {2008-06-30 19:47:43 +0200},
	Doi = {10.1023/A:1015617019423},
	Journal = {Cluster Computing},
	Keywords = {grid, scheduling},
	Month = {July},
	Number = {3},
	Pages = {237--246},
	Priority = {2},
	Title = {{Condor-G: A Computation Management Agent for Multi-Institutional Grids}},
	Url = {http://dx.doi.org/10.1023/A:1015617019423},
	Volume = {5},
	Year = {2002},
	Bdsk-Url-1 = {http://dx.doi.org/10.1023/A:1015617019423},
	Bdsk-Url-2 = {http://dx.doi.org/10.1023/A:1015617019423}}

@article{condor-g-short,
	Author = {Frey, J. and Tannenbaum, T. and Livny, M. and Foster, I. and Tuecke, S.},
	Citeulike-Article-Id = {291860},
	Date-Added = {2008-02-28 10:08:47 -0600},
	Date-Modified = {2008-06-30 19:47:43 +0200},
	Doi = {10.1023/A:1015617019423},
	Journal = {Cluster Computing},
	Keywords = {grid, scheduling},
	Month = {July},
	Number = {3},
	Pages = {237--246},
	Priority = {2},
	Title = {{Condor-G: A Computation Management Agent for Multi-Institutional Grids}},
	Volume = {5},
	Year = {2002},
	Bdsk-Url-1 = {http://dx.doi.org/10.1023/A:1015617019423}}

@inproceedings{1362680,
	Address = {New York, NY, USA},
	Author = {Ioan Raicu and Yong Zhao and Catalin Dumitrescu and Ian Foster and Mike Wilde},
	Booktitle = {SC '07: Proceedings of the 2007 ACM/IEEE conference on Supercomputing},
	Date-Added = {2008-08-09 21:04:33 +0200},
	Date-Modified = {2008-08-09 21:04:53 +0200},
	Doi = {http://doi.acm.org/10.1145/1362622.1362680},
	Isbn = {978-1-59593-764-3},
	Location = {Reno, Nevada},
	Pages = {1--12},
	Publisher = {ACM},
	Title = {{Falkon: A Fast and Light-Weight TasK ExecutiON Framework}},
	Year = {2007},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/1362622.1362680}}

@article{Wilde2011,
	Abstract = {Scientists, engineers, and statisticians must execute domain-specific application programs many times on large collections of file-based data. This activity requires complex orchestration and data management as data is passed to, from, and among application invocations. Distributed and parallel computing resources can accelerate such processing, but their use further increases programming complexity. The Swift parallel scripting language reduces these complexities by making file system structures accessible via language constructs and by allowing ordinary application programs to be composed into powerful parallel scripts that can efficiently utilize parallel and distributed resources. We present Swift's implicitly parallel and deterministic programming model, which applies external applications to file collections using a functional style that abstracts and simplifies distributed parallel execution.},
	Author = {Michael Wilde and Mihael Hategan and Justin M. Wozniak and Ben Clifford and Daniel S. Katz and Ian Foster},
	Doi = {10.1016/j.parco.2011.05.005},
	Issn = {0167-8191},
	Journal = {Parallel Computing},
	Keywords = {Dataflow},
	Note = {Emerging Programming Paradigms for Large-Scale Scientific Computing},
	Number = {9},
	Pages = {633--652},
	Title = {Swift: A language for distributed parallel scripting},
	Url = {http://www.sciencedirect.com/science/article/pii/S0167819111000524},
	Volume = {37},
	Year = {2011},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0167819111000524},
	Bdsk-Url-2 = {http://dx.doi.org/10.1016/j.parco.2011.05.005}}

@misc{coasters,
	Howpublished = {\url{http://wiki.cogkit.org/wiki/Coasters}},
	Key = {coasters},
	Title = {Coasters},
	Year = 2009}

@misc{topos,
	Howpublished = {\url{https://grid.sara.nl/wiki/index.php/Using_the_Grid/ToPoS}},
	Key = {Topos},
	Title = {ToPoS - A Token Pool Server for Pilot Jobs},
	Year = 2011}

@inproceedings{1652061,
	Author = {Walker, E. and Gardner, J.P. and Litvin, V. and Turner, E.L.},
	Booktitle = {Challenges of Large Applications in Distributed Environments, 2006 IEEE},
	Doi = {10.1109/CLADE.2006.1652061},
	Keywords = {Condor;NSF TeraGrid;Sun Grid Engine cluster;cooperative system;distributed computing environment;personal adaptive cluster;resource management;scientific job management;grid computing;resource allocation;workstation clusters;},
	Month = {0-0},
	Pages = {95-103},
	Title = {Creating personal adaptive clusters for managing scientific jobs in a distributed computing environment},
	Year = {2006},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/CLADE.2006.1652061}}

@article{10.1109/HPC.2000.846563,
	Address = {Los Alamitos, CA, USA},
	Author = {Rajkumar Buyya and David Abramson and Jonathan Giddy},
	Doi = {http://doi.ieeecomputersociety.org/10.1109/HPC.2000.846563},
	Isbn = {0-7695-0589-2},
	Journal = {International Conference on High-Performance Computing in the Asia-Pacific Region},
	Pages = {283-289},
	Publisher = {IEEE Computer Society},
	Title = {Nimrod/G: An Architecture for a Resource Management and Scheduling System in a Global Computational Grid},
	Volume = {1},
	Year = {2000},
	Bdsk-Url-1 = {http://doi.ieeecomputersociety.org/10.1109/HPC.2000.846563}}

@article{1742-6596-219-6-062041,
	Abstract = {The Panda Workload Management System is designed around the concept of the Pilot Job -- a "smart wrapper" for the payload executable that can probe the environment on the remote worker node before pulling down the payload from the server and executing it. Such design allows for improved logging and monitoring capabilities as well as flexibility in Workload Management. In the Grid environment (such as the Open Science Grid), Panda Pilot Jobs are submitted to remote sites via mechanisms that ultimately rely on Condor-G. As our experience has shown, in cases where a large number of Panda jobs are simultaneously routed to a particular remote site, the increased load on the head node of the cluster, which is caused by the Pilot Job submission, may lead to overall lack of scalability. We have developed a Condor-inspired solution to this problem, which is using the schedd-based glidein, whose mission is to redirect pilots to the native batch system. Once a glidein schedd is installed and running, it can be utilized exactly the same way as local schedds and therefore, from the user's perspective, Pilots thus submitted are quite similar to jobs submitted to the local Condor pool.},
	Author = {Po-Hsiang Chiu and Maxim Potekhin},
	Journal = {Journal of Physics: Conference Series},
	Number = {6},
	Pages = {062041},
	Title = {Pilot factory -- a Condor-based system for scalable Pilot Job generation in the Panda WMS framework},
	Url = {http://stacks.iop.org/1742-6596/219/i=6/a=062041},
	Volume = {219},
	Year = {2010},
	Bdsk-Url-1 = {http://stacks.iop.org/1742-6596/219/i=6/a=062041}}

@misc{dpa_surveypaper,
	Note = {S. Jha et al., {\em Programming Abstractions for Large-scale Distributed Application s}, Submitted to ACM Computing Surveys; draft at \url{http://www.cct.lsu.edu/~sjha/publications/dpa_surveypaper.pdf}}}

@misc{egi,
	Author = {{EGI}},
	Howpublished = {\url{http://www.egi.eu/}},
	Key = {egi},
	Year = {2012}}

@article{bfast2009,
	Author = {Homer, N. and Merriman, B. and Nelson, S. F.},
	Journal = {PLoS One},
	Number = {11},
	Pages = {e7767},
	Title = {{BFAST : An alignment tool for large scale genome resequencing}},
	Volume = {4},
	Year = {2009}}

@misc{saga_rm,
	Author = {Andre Merzky},
	Booktitle = {OGF Draft},
	Howpublished = {\url{https://svn.cct.lsu.edu/repos/saga-ogf/trunk/documents/saga-package-resource/}},
	Title = {{SAGA Resource Management API (DRAFT)}},
	Year = 2011}

@misc{saga_advert,
	Author = {Andre Merzky},
	Howpublished = {OGF Document Series 177, \url{http://www.gridforum.org/documents/GFD.177.pdf}},
	Title = {{SAGA API Extension: Advert API}},
	Year = 2011}

@misc{redis,
	Author = {{Redis}},
	Howpublished = {\url{http://redis.io/}},
	Key = {Redis},
	Year = 2012}

@misc{zmq,
	Author = {{ZeroMQ}},
	Howpublished = {\url{http://www.zeromq.org/}},
	Key = {zmq},
	Year = 2012}

@misc{fg,
	Howpublished = {\url{https://portal.futuregrid.org/}},
	Key = {FutureGrid},
	Title = {{FutureGrid: An Experimental, High-Performance Grid Test-bed}},
	Year = 2012}

@misc{xsede,
	Howpublished = {\url{https://www.xsede.org/}},
	Key = {XSEDE},
	Title = {{XSEDE: Extreme Science and Engineering Discovery Environment}},
	Year = 2012}

@article{Gelernter:1985:GCL:2363.2433,
	Acmid = {2433},
	Address = {New York, NY, USA},
	Author = {Gelernter, David},
	Doi = {http://doi.acm.org/10.1145/2363.2433},
	Issn = {0164-0925},
	Issue = {1},
	Journal = {ACM Trans. Program. Lang. Syst.},
	Month = {January},
	Numpages = {33},
	Pages = {80--112},
	Publisher = {ACM},
	Title = {Generative communication in Linda},
	Url = {http://doi.acm.org/10.1145/2363.2433},
	Volume = {7},
	Year = {1985},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/2363.2433}}

@misc{loni,
	Author = {{LONI}},
	Howpublished = {\url{http://www.loni.org}},
	Key = {loni},
	Year = {2011}}

@article{Moscicki20092303,
	Abstract = {In this paper, we present the computational task-management tool Ganga, which allows for the specification, submission, bookkeeping and post-processing of computational tasks on a wide set of distributed resources. Ganga has been developed to solve a problem increasingly common in scientific projects, which is that researchers must regularly switch between different processing systems, each with its own command set, to complete their computational tasks. Ganga provides a homogeneous environment for processing data on heterogeneous resources. We give examples from High Energy Physics, demonstrating how an analysis can be developed on a local system and then transparently moved to a Grid system for processing of all available data. Ganga has an API that can be used via an interactive interface, in scripts, or through a GUI. Specific knowledge about types of tasks or computational resources is provided at run-time through a plugin system, making new developments easy to integrate. We give an overview of the Ganga architecture, give examples of current use, and demonstrate how Ganga can be used in many different areas of science.
Program summary
Program title: Ganga

Catalogue identifier: AEEN_v1_0

Program summary URL: http://cpc.cs.qub.ac.uk/summaries/AEEN_v1_0.html

Program obtainable from: CPC Program Library, Queen's University, Belfast, N. Ireland

Licensing provisions: GPL

No. of lines in distributed program, including test data, etc.: 224 590

No. of bytes in distributed program, including test data, etc.: 14 365 315

Distribution format: tar.gz

Programming language: Python

Computer: personal computers, laptops

Operating system: Linux/Unix

RAM: 1 MB

Classification: 6.2, 6.5

Nature of problem: Management of computational tasks for scientific applications on heterogenous distributed systems, including local, batch farms, opportunistic clusters and Grids.

Solution method: High-level job management interface, including command line, scripting and GUI components.

Restrictions: Access to the distributed resources depends on the installed, 3rd party software such as batch system client or Grid user interface.},
	Author = {J.T. Moscicki {\it et al}},
	Doi = {10.1016/j.cpc.2009.06.016},
	Issn = {0010-4655},
	Journal = {Computer Physics Communications},
	Keywords = {Application configuration},
	Number = {11},
	Pages = {2303 - 2316},
	Title = {Ganga: A tool for computational-task management and easy access to Grid resources},
	Url = {http://www.sciencedirect.com/science/article/pii/S0010465509001970},
	Volume = {180},
	Year = {2009},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0010465509001970},
	Bdsk-Url-2 = {http://dx.doi.org/10.1016/j.cpc.2009.06.016}}

@article{Moscicki20092303-long,
	Abstract = {In this paper, we present the computational task-management tool Ganga, which allows for the specification, submission, bookkeeping and post-processing of computational tasks on a wide set of distributed resources. Ganga has been developed to solve a problem increasingly common in scientific projects, which is that researchers must regularly switch between different processing systems, each with its own command set, to complete their computational tasks. Ganga provides a homogeneous environment for processing data on heterogeneous resources. We give examples from High Energy Physics, demonstrating how an analysis can be developed on a local system and then transparently moved to a Grid system for processing of all available data. Ganga has an API that can be used via an interactive interface, in scripts, or through a GUI. Specific knowledge about types of tasks or computational resources is provided at run-time through a plugin system, making new developments easy to integrate. We give an overview of the Ganga architecture, give examples of current use, and demonstrate how Ganga can be used in many different areas of science.
Program summary
Program title: Ganga

Catalogue identifier: AEEN_v1_0

Program summary URL: http://cpc.cs.qub.ac.uk/summaries/AEEN_v1_0.html

Program obtainable from: CPC Program Library, Queen's University, Belfast, N. Ireland

Licensing provisions: GPL

No. of lines in distributed program, including test data, etc.: 224 590

No. of bytes in distributed program, including test data, etc.: 14 365 315

Distribution format: tar.gz

Programming language: Python

Computer: personal computers, laptops

Operating system: Linux/Unix

RAM: 1 MB

Classification: 6.2, 6.5

Nature of problem: Management of computational tasks for scientific applications on heterogenous distributed systems, including local, batch farms, opportunistic clusters and Grids.

Solution method: High-level job management interface, including command line, scripting and GUI components.

Restrictions: Access to the distributed resources depends on the installed, 3rd party software such as batch system client or Grid user interface.},
	Author = {J.T. Moscicki and F. Brochu and J. Ebke and U. Egede and J. Elmsheuser and K. Harrison and R.W.L. Jones and H.C. Lee and D. Liko and A. Maier and A. Muraru and G.N. Patrick and K. Pajchel and W. Reece and B.H. Samset and M.W. Slater and A. Soroko and C.L. Tan and D.C. van der Ster and M. Williams},
	Doi = {10.1016/j.cpc.2009.06.016},
	Issn = {0010-4655},
	Journal = {Computer Physics Communications},
	Keywords = {Application configuration},
	Number = {11},
	Pages = {2303 - 2316},
	Title = {Ganga: A tool for computational-task management and easy access to Grid resources},
	Url = {http://www.sciencedirect.com/science/article/pii/S0010465509001970},
	Volume = {180},
	Year = {2009},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0010465509001970},
	Bdsk-Url-2 = {http://dx.doi.org/10.1016/j.cpc.2009.06.016}}

@misc{leaky_abstractions,
	Author = {{Joel Spolsky}},
	Note = {\url{http://www.joelonsoftware.com/articles/LeakyAbstractions.html}},
	Title = {{The Law of Leaky Abstractions}}}
