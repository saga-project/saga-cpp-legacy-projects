\documentclass[conference,final]{IEEEtran}

\usepackage{latex8}
\usepackage{times}

\usepackage[utf8]{inputenc}
\usepackage{url}
\usepackage{float}
\usepackage{times}    
\usepackage{multirow}    
\usepackage{listings}   
\usepackage{times}     
\usepackage{paralist}    
\usepackage{wrapfig}    
\usepackage[small,it]{caption}
\usepackage{multirow}
\usepackage{ifpdf}
%\usepackage{srcltx}
\usepackage{subfigure}
\usepackage{paralist}

\usepackage{listings}
\usepackage{keyval}  
\usepackage{color}
\definecolor{listinggray}{gray}{0.95}
\definecolor{darkgray}{gray}{0.7}
\definecolor{commentgreen}{rgb}{0, 0.4, 0}
\definecolor{darkblue}{rgb}{0, 0, 0.4}
\definecolor{middleblue}{rgb}{0, 0, 0.7}
\definecolor{darkred}{rgb}{0.4, 0, 0}
\definecolor{brown}{rgb}{0.5, 0.5, 0}

\newif\ifdraft
\drafttrue
\ifdraft
\newcommand{\jhanote}[1]{ {\textcolor{red} { ***shantenu: #1 }}}
\newcommand{\alnote}[1]{ {\textcolor{blue} { ***andre: #1 }}}
\newcommand{\smnote}[1]{ {\textcolor{green} { ***sharath: #1 }}}
\newcommand{\msnote}[1]{ {\textcolor{cyan} { ***mark: #1 }}}
\newcommand{\note}[1]{ {\textcolor{magenta} { ***Note: #1 }}}
\else
\newcommand{\alnote}[1]{}
\newcommand{\athotanote}[1]{}
\newcommand{\smnote}[1]{}
\newcommand{\jhanote}[1]{}
\newcommand{\msnote}[1]{}
\newcommand{\note}[1]{}
\fi

\lstdefinestyle{myListing}{
  frame=single,   
  backgroundcolor=\color{listinggray},  
  %float=t,
  language=C,       
  basicstyle=\ttfamily \footnotesize,
  breakautoindent=true,
  breaklines=true
  tabsize=2,
  captionpos=b,  
  aboveskip=0em,
  belowskip=-2em,
  %numbers=left, 
  %numberstyle=\tiny
}      

\lstdefinestyle{myPythonListing}{
  frame=single,   
  backgroundcolor=\color{listinggray},  
  %float=t,
  language=Python,       
  basicstyle=\ttfamily \footnotesize,
  breakautoindent=true,
  breaklines=true
  tabsize=2,
  captionpos=b,  
  %numbers=left, 
  %numberstyle=\tiny
}

\newcommand{\up}{\vspace*{-1em}}
\newcommand{\upp}{\vspace*{-0.5em}}
\newcommand{\numrep}{8 }
\newcommand{\samplenum}{4 }
\newcommand{\tmax}{$T_{max}$ }
\newcommand{\tc}{$T_{C}$ }
\newcommand{\tcnsp}{$T_{C}$}
\newcommand{\bj}{BigJob}

% This is now the recommended way for checking for PDFLaTeX:
\usepackage{ifpdf}

%\newif\ifpdf
%\ifx\pdfoutput\undefined
%\pdffalse % we are not running PDFLaTeX
%\else
%\pdfoutput=1 % we are running PDFLaTeX
%\pdftrue
%\fi

\ifpdf
\usepackage[pdftex]{graphicx}
\else
\usepackage{graphicx}
\fi

% \title{Towards A Framework for Pilot-Abstractions for Production
%   Cyberinfrastructure}

\title{A Framework for Pilot-Abstractions for Dynamic Execution}

% \jhanote{Alternate title: The Tiered Resource OverlaY framework
%   (TROY): An Empirical Framework for Pilot-* Abstractions}
% 
% \jhanote{old title: TROY -- Tiered Resource Overlay Framework: Towards
%   a Framework for Pilot-Abstractions for Distributed
%   Cyberinfrastructure}


\date{}

\begin{document}

\ifpdf
\DeclareGraphicsExtensions{.pdf, .jpg, .tif}
\else
\DeclareGraphicsExtensions{.eps, .jpg}
\fi

\author{
  Andr\'e Luckow$^{1}$, Mark Santcroos$^{2,1}$, Sharath Maddineni$^{1}$, Shantenu Jha$^{3,1*}$\\
  \small{\emph{$^{1}$Center for Computation \& Technology, Louisiana State University, USA}}\\
 \small{\emph{$^{2}$Bioinformatics Laboratory, Academic Medical Center, University of Amsterdam, The Netherlands}}\\
 \small{\emph{$^{3}$ Rutgers University, Piscataway, NJ 08854, USA}}\\
  \small{\emph{$^{*}$Contact Author: \texttt{shantenu.jha@rutgers.edu}}}\\
  \up\up\up\up }

\maketitle

\begin{abstract}
  Distributed Cyberinfrastructure and Applications require dynamical
  resource utilization models and not static resources.  Pilot-Jobs
  have been one notable success -- in the scope of usage and number of
  CI that support them and applications that use them.  However, in
  spite of broad uptake, there does not exist a well defined, unifying
  theoretical framework for Pilot-Job using which different
  implementations can be compared, contrasted and defined. This paper
  is an attempt to (i) provide a minimal but complete model/framework
  of Pilot-Jobs, (ii) extend the basic framework from compute jobs to
  data, (iii) introduce TROY (tiered resource overlay) as an
  implementation of this framework using SAGA, i.e., consistent with
  its API, job-model etc., (iv) mapping DIANE -- an existing and well
  known PJ -- to P*, we establish the generality of the
  model/framework (P*), (v) establish and validate the implementation
  of the TROY API by concurrently using BigJob and DIANE across
  multiple infrastructures.
\note{introduce P*}
\end{abstract}

\section*{Outline}

\jhanote{Need consistency in the way Pilot-Job is written: currently
  we have pilot-job, Pilot-Job, pilot job, and possibly more...}
\alnote{Would Pilot-Job and Pilot-Data be ok?}

% \begin{footnotesize}
% \begin{verbatim}
% 1. Introduction:
% - Distributed CI and the need/role of Dynamic 
% Execution 
% - PJs as an effective abstraction for DE
% - Brief Overview of the status of PJ
% -- Many PJ out there but no consistent terminology, 
% framework to compare/contrast
% - Basis for extension of Pilot concepts to 
% other dimensions
 
% 2. A Conceptual FW for Pilot-abstractions for DE

% 3 TROY: SAGA-based implementation of Pilot-abstractions

% TROY = Pilot-Job (BigJob) + Pilot-Data (BigData)

% 4 Analysing other PJs using the Conceptual FW

% 5 Experiments/Implementation/Comparision:
% - PJ interop using the TROY API using DARE
% -- this is where DIANE, BigJob will be used together
% using TROY API (effectively just BigJob)
% -- all other experiments, measurements 
% and validation tests

% \end{verbatim}
% \end{footnotesize}

The primary objectives of this work are:

\begin{enumerate}

\item Establish the need for dynamic execution of applications -
  distributed as well as high-end performance.

\item We define the basic characteristics of the dynamic apps and we
  understand the requirements of dynamic apps need to do in a
  distributed environment.

\item We understand the capability that must be provided by the
  infrastructure to support these application

\item We describe the pilot-job as a good prototype of an abstraction
  that supports dynamic execution

\item We define the characteristics that need to be supported by a
  pilot-job \jhanote{Infrastructure or Application characteristics?}
  \alnote{I think we meant application characteristics}

\item There exist multiple PJ implementations out there but no way to
  compare and contrast. Provide a framework to aid an understanding of
  pilot-jobs and the ability to compare, contrast and understand
  different pilot-jobs.  Provide both a theoretical and empirically
  useful approach to determining which PJ to use

\item Empirical implementation of TRoY and demonstration of
  concurrent/interoperation between equivalent but distinct Pilot-Job
  implementation. Highlight unique feature of Troy: User extensible
  and customisable. \jhanote{We should talk about this in light of the
    reviews of the paper with Bishop}
\end{enumerate}

Points 1-3 can go into the beginning of \S 2.  Points 4 \& 5 should be
addressed in both introduction (see one of the \jhanote{} above), as
well as in the beginning of \S 2. Points 6, 7 are addressed in the
Introduction.

\section{Introduction and Overview}


Distributed infrastructure almost by definition is comprised of a set
of resources that are fluctuating -- growing, shrinking, changing in
load and capability.  The ability to respond to such changes is thus
an important attribute of any application that wants to utilize
distributed infrastructure efficiently, as opposed to a static
execution model characteristic of parallel and cluster computing.

% The ability to use resources dynamically
% and how they are provisioned and federated, it is {\it ipso facto}
% determined that

In addition, the evolution or internal dynamics of an application may
vary, thereby changing the resource requirement or utilization
ability; this has the resultant effect of an application being
dynamic, though dynamic resource execution maybe just one of multiple
responses.  For example, different solvers, or granularity of existing
solvers, adaptive algorithms and/or implementations, can also provide
applications with an agile execution model, effectively equivalent to
dynamic resource utilization. The two approaches can be viewed as
different approaches to the dynamic resource problem: in one, the
supply is kept fixed and the demand is changed to meet the supply; in
the other the supply changes in response to the demand.

For the purposes of this paper, we will focus on the latter, wherein
the availability and utilization of resources changes...  As a
consequence, we will not consider dynamic execution arising from
internal application changes.  In other words, we will concern
and confine ourselves to  % distributed
% cyberinfrastructure and applications 
the case where applications have the ability to respond to a
fluctuating resource pool, i.e., the set of resources utilized at
$T=$1 is not the same at $T=$0.  Typically, this will involve a
single ``large'' distributed application, that is comprised of many
smaller tasks.

% Additionally, existing development and execution models and
% abstractions are mostly remnants of cluster and high-performance
% computing -- which almost by definition impose and imply a static
% resource utilization model. \jhanote{Define/elaborate on static
%   resource model.}  

% At this point, it is worth elaborating upon dynamic resource
% utilization;

\alnote{not sure why do we need week and strong model? Determinism and 
distributed systems don't go together well. Also, I wouldn't claim that
the prob. of adv. rerv. is 1 (it might be 0.99...). PJ are more in line
with the weak model. Do we say that Advance Reserv. are better?}
There exist two models of dynamic resource utilization, which we refer
to as the weak (probabilistic) model of Dynamic Execution (DE) and the
strong (deterministic) model of DE. In the former, there exists a
certain probability to acquire a resource at a given instant of time;
in the latter the probability of acquiring a resource is by definition
is 1, but there exists a broad range of times over which this
probability will first be 1.  Multiple approaches exist to support
dynamic resource utilization -- both weak and strong.  For example,
Advanced Scheduling (without pre-emption) provides a guarantee
(probability of 1) at a sufficiently far out time window.

Another common approach is the Pilot-Job approach, wherein there is a
decoupling between workload management and resource
assignment/scheduling.  Along with empirical evidence, our experience
suggests that distributed applications that are able to use tools,
abstractions and services that break the coupling between workload
management and resource assignment/scheduling have been more
successful at efficiently utilizing distributed resources.  In
addition, the ability to provide these capabilities in user-space, leads
to the basic concept of a pilot-job.

% \jhanote{Candidate for removal: On reflection, distributed
%   applications that strongly and statically (early) bind to resources,
%   have been less successful at efficiently utilizing distributed
%   resources, as well are unlikely to be able to scale -- either due to
%   hindrances arising from failures, unpredictable system loads and or
%   changing application requirements and resource availability, amongst
%   other factors.}

Interestingly there exists many implementations of the Pilot-Job
abstraction, wherein different projects and users have rolled-out
their own. The fact that users have voted with their feet for
Pilot-Jobs, reinforces the fact that the Pilot-Job is both an useful
and correct abstraction for distributed cyberinfrastructure; the fact
that it has become an ``unregulated cottage industry'' reaffirms the
lack of common nomenclature, integration, interoperability and
extension.

Our work is motivated by the existing status of the usage and
availability of the Pilot-Job abstraction vis-a-vis the current
landscape of distributed applications and cyberinfrastructure.

%\subsection*{Motivation}

% \jhanote{Not sure yet if we should have ``Motivation and Aims'' as a
%   stand alone section, and if so how to name that section....or if we
%   should wrap it into \S 1..}



% The real power of distributed systems, however, arises from adaptive
% algorithms and implementations that provide applications with an agile
% execution model, and thus the ability to use resources dynamically as
% opposed to a static execution model inherited from parallel and
% cluster computing



% \jhanote{refine erstwhile motivation} Distributed cyberinfrastructure
% are inherently dynamic: resources can suddenly fail or new resources
% can become available at any time. Generally there are two types of
% dynamism.

% \begin{itemize}
% \item \textbf{Resource dynamism:} Distributed systems are inherently dynamic
%      -- resources can become available or fail at any time. The same holds for
%       network connections. Further, hybrid infrastructures comprised of 
%       different resources classes can vary significantly in their costs for 
%       usage, performance, availability and the guarantees for quality of service 
%       they provide.
      
% \item \textbf{Application dynamism} describes the requirement of many
% applications to support dynamic resource requirements. An example are
% applications whose execution time resource requirements cannot be determined
% exactly in advance (either due to changes in runtime requirements) or those that
% are dependent on dynamic data (e. g. sensor, in-transit or variable source/sink
% of data). Further, the application requirements may change, due to,
% for example, a failure or an application event.
% \end{itemize}

% Further things to consider:
% \begin{itemize}
%     \item dynamic scheduling
%     \item dynamic task placement
%     \item autonomic behaviors: Monitoring of the system/application state and 
%     adaptations of the application and/or resources to respond to changing 
%     requirements or environment.
%   \item Multi-level and multi-dimensional scheduling (in a distributed
%     context). Make reference to it.
% \end{itemize}


%\section*{Aims} 


To achieve our objectives, we begin this work with an attempt to
provide a minimal, but complete model -- for the Pilot-Job
abstraction, to provide a common and consistent framework to compare
and contrast different Pilot-Jobs implementations/frameworks. This is,
to the best of our knowledge, the first such attempt.

A natural and logical extension of the Pilot-Job model, arising from
the need to treat data as a first-class schedulable entity, is a
concept analogous to Pilot-Job: the Pilot-Data. Given the consistent
treatment of data and compute, as potentially equal components in a
framework to support dynamic resource and execution, we refer this
model as the P-* Model ("P-star").

In \S3 we introduce TROY -- A Tiered Resource Overlay framework, as an
implementation of the P-* Model using the SAGA API. The Pilot-Job and
Pilot-Data concepts in TROY are referred to as BigJob and BigData. As
we will discuss, consistent with the goals and aims of SAGA, there can
be multiple {\it atomic} instances of BigJob for different backends.
Thus we posit that the TROY API is a general purpose API that can be
used for all Pilot-Jobs. Before validating this claim, in \S4 of this
paper, we discuss the mapping of DIANE to the P-* Model and outline
briefly how other well known Pilot-Jobs can be understood using the
{\it vectors}~\cite{dpa_surveypaper} of the P-* Model.

In \S5 we validate the TROY API by demonstrating how DIANE -- an
existing and widely used Pilot-Job, can be given a well defined API
via the TROY-BigJob API. To further substantiate the impact of
TROY-BigJob API, we will demonstrate interoperability between
different Pilot-Jobs -- a native SAGA based Pilot-Job, referred to as
BigJob/SAGA, and DIANE.  We believe this is also the first
demonstration of interoperation of different Pilot-Jobs.

% \jhanote{Should we to introduce Dynamic Applications explicitly in the
%   title? Just a question, not a suggestion...}

%\subsection{Introduction \jhanote{SJ, AL}}


%\section{Pilot-* : An abstraction for Dynamic Execution}
%\section{TROY: A Model of Pilot-Abstractions for Dynamic Execution}

\section{P-* Model: A Conceptual Framework for Pilot-Abstractions for
  Dynamic Execution}
\label{sec:pilot-model}


In general, there exist multiple reasons why distributed applications
have not been able to utilize distributed cyberinfrastructure
effectively and without immense effort~\cite{dpa_surveypaper}.  At the
root of the problem is the fact that developing large-scale
distributed applications is fundamentally a difficult process. The
range of proposed tools, programming systems and environments is
bewildering large, making integration, extensibility and
interoperability difficult.
 
Additionally, the uptake of distributed infrastructures by scientific
applications has been limited by the availability of extensible,
pervasive and simple-to-use abstractions which are required at
multiple levels â€“ development, deployment and execution stages of
scientific applications.

% Specifically, Pilot-Jobs support the decoupling of workload
% submission from resource assignment; this results in a flexible
% execution model, which in turn enables the distributed scale-out of
% applications on multiple and possibly heterogeneous resources.

The Pilot-Job abstraction has been shown to be an effective
abstraction to address many requirements of distributed scientific
applications. Although they are all functionally equivalent -- they
support the decoupling of workload submission from resource
assignment, it is often impossible to use them interoperably --- most
Pilot-Job implementations however, are tied to a specific
infrastructure, or even to compare and contrast them.

In an attempt to provide a common analytical framework using which
most, if not all commonly used pilot-jobs can be understood, we
present the P-* Model of Pilot-Abstractions. The P-* Model is derived
from an analysis of many pilot-jobs; based upon this analysis, we
first present the common {\it elements} of the P-* Model, followed by
a description of the properties that must be assigned and that
determine the interaction of these elements and the overall
functioning of any pilot-job based upon the P-* Model.

\subsection{Elements of the P-* Model}

\alnote{Remark from AndreM: we need to careful when talking about
  Pilot-Job as a framework and Pilot-Job as the placeholder job
  (agent).}

\begin{compactenum}[A.]

\item \textbf{Pilot-Job Framework:} The PJ framework provides the
  abstraction of a container for multiple Unit of Works that may be
  dynamically added to it independently from the underlying resource
  pool.  It provides a mechanism to decouple unit of works (the
  compute ``tasks'') from being hard-coded to a specific ``resource''
  or delay the binding.

\item \textbf{Pilot-Job:} The PJ (also referred to as pilot) is the
  entity that actually gets submitted and scheduled on a resource. 
  Commonly, the PJ utilizes an agent to manage the
  set of allocated resources.

\item \textbf{Unit of Work (UW):} A unit of work is the workload that
  encapsulates a self-contained piece of work and will be assigned to
  a resource indirectly via a Pilot-Job.  The PJ in turn has
  flexibility in determining when and how-many resources the UW will
  receive.

\item \textbf{Unit of Scheduling (US):} are entities 
  that get scheduled to the resource, and to which UWs are assigned
  (inside the PJ framework). A sub-job runs an instance of the application
  kernel~\ref{application-program}.

% are the unit of works that are sent down to \ref{pilot-agent} to be
%   executed.  Sub-jobs/jobs are used to reserve the resources required
%   by a task.
%\jhanote{I changed the US description significantly} 

\item \textbf{Pilot-Manager:} The element responsible for coordinating
  the different UW, US and other elements.

% \item \textbf{Resource:} A storage/compute resource that has a common
%   entry point (like a queue). Commonly, a resource is accessed by a 
%   LRM. \alnote{move to impl. section} \label{resource}

\item \textbf{Application:} is upper layer on the stack
  e.g. DARE-NGS. The application utilizes the BigJob API to execute
  instances of \ref{application-program}. \label{application}

\item \textbf{Application Kernel:} An application kernel is actual
  binary that gets run, e.g. \texttt{/usr/local/bin/bfast}
  (Alternative terms: Program, Executable,
  Software). \label{application-program}
	
\end{compactenum}

 

% \textbf{Diane Definition of Terms: } The computation consists of many worker
% processes which communicate with one master process (the worker processes do not
% need to share the filesystem nor memory). The ensemble of computation is called
% a run and it consists of many tasks which may be executed in parallel. A task is
% defined as a set of parameters which are produced by the RunMaster (running on a
% master node) and consumed by the WorkerAgent (running on a worker node).
% 
% from 
% 
% DIANE assumes the master-worker computing model (Fig.
% 1). Client sends job parameters to the Planner which partitions
% the job into smaller tasks executed by the Workers. Integrator
% is responsible for merging the results of task execution and
% sending the final job outcome to the Client.


\subsection{Characteristics of P*-Model:}
\label{sec:p_star_elements}

As we will see in \S4, the above elements can be mapped to specific
entities in many pilot-jobs in existence and use.  But in order to
understand the degrees of freedom that any specific pilot-job
implementation must constraint as well as the functioning of
pilot-jobs implementation, we propose a set of fundamental
properties/characteristics. These characteristics are integral
components of the P-* Model, in that they describe the interactions
between the elements, and thus aid in the description of P*
frameworks. 

% Further, these properties are important for the implementation of
% P-*.  list several characteristics.

\textbf{Coordination:} describes how the various components of the
pilot-job framework are internally coordinated. Coordination between
different elements is required to understand:

\begin{compactitem}
\item \textbf{Decision Making:} In the \textbf{central} model
  information about available resources (aka pilots) are collected by
  a central manager.  Decisions are centrally made by a manager
  process, which decides which UW is executed on what resource. In the
  \textbf{hierarchical} model the decision making process is divided
  up into a hierarchy of distributed agents. Each of them coordinates
  a coordinates a defined aspect (e.\,g.\ a certain set of
  resources). In the \textbf{decentral} model, control is distributed
  among the different components. Pilot-Jobs with decentralized
  decision making often utilize agents that accepts respectively pull
  UWs according to a set of defined criteria.

\item \textbf{Push versus Pull Model:} We define the terms push and
  pull based upon the determinism of the binding. If the task is
  centrally explicitly addressed to a specific pilot, and there is
  thus no freedom for a pilot to select the task, we speak of a push
  model. In all other situations, when there is a degree of freedom
  for the pilots to select a task, we speak of pull. This model is
  applicable to different aspects of the pilot-job framework (e.g. to
  task binding, resource binding, resource additions/removals).
  \alnote{should we move push/pull to communication? It's is also
    currently references there already}

\end{compactitem}

Commonly, distributed coordination mechanisms, such as master-worker
or a set of distributed software agents are utilized.

    % \item \textbf{Coordination} describes how the various components of 
    % the Pilot-Job framework are coordinated. Possible values for this vector 
    % include data flow, control flow, SPMD (where the control flow is implicit in 
    % all copies of the single program), master-worker (the work done by the 
    % workers is controlled by the master). 
    % \begin{itemize}
    %   \item The data flow describes the flow of messages between the 
    %   components of the framework, e.\,g.\ point-to-point messaging or 
    %   publish-subscribe (see Communication).
    %   \item The control flow describes how the various components of the
    %    framework are managed. In the context of pilot-jobs this particularly 
    %    means how the resource on which a sub-job is executed is determined. 
	% \item Sub-Job pull vs. push applies to control and data flow (different 
	% objects that are pushed/pulled)

\textbf{Communication:} describes the mechanisms for data exchange
between the components of the framework, can be used (e.\,g.\
point-to-point, all-to-all, one-to-all, all-to-one, or
group-to-group), stream (potentially unicast or multicast),
publish/subscribe or shared data spaces. Shared data spaces (e.\,g.\
tuple spaces) as used by BigJob are associated with a push/pull
coordination schema, i.e. the central manager pushes something to the
shared data space and the agents periodically check for new data.
		
% \textbf{Dynamic Resources:} Dynamic resources describes the ability 
% 	to acquire and release resources at runtime of the pilot-job.
% 	    \alnote{By definition a BJ should be dynamic. After acquiring additional 
% 	    resources, the external coordination is the same for both BJ.}

\textbf{UW Binding} defines how and at what time the assignment of a
UW to a Pilot-Job is done.  For example, a UW can be bound to a
Pilot-Job either before the Pilot-Job has in turn been scheduled (late
binding), whereas late-late binding can occur if the UW is bound after
the Pilot-Job has been assigned to a resource.  Depending upon the
type of binding supported, the workload is either specified completely
independent of a (specific) resource or just partially.
\alnote{Question: Is the time of binding independent of wether the PJ
  has resources or not? Late-late binding can also occur if a PJ has
  resources.}  Furthermore, the time of binding describes the point at
which the binding decision is made, e.\,g.\ in the case of early
binding, the decision could be made by the application, while in late
binding mode the decision is made by the middleware
framework. 

    %   \begin{itemize}
    %       \item In the traditional early binding approach a job carries
    %        exactly one task which is specified at a time of job submission.
    %       \item Partial binding?
    %       \item Late binding is a scheduling and coordination
    %        method, where work is assigned to a job at runtime rather than at
    %        submission time.
    %       \begin{itemize}
    %           \item At sub-job submission 
    %           \item After sub-job submission
    %       \end{itemize}
    % \end{itemize}

% \jhanote{Difference between mapping and assignment}\alnote{in this
%   case both terms are used very synonymous.}

\textbf{UW Scheduling:} Task scheduling describes i) the process of
assigning UW to US (pilot internal) and ii) of assigning US to physical
resources (pilot external).  The different scheduling decisions that
need to be made are representative of multi-level scheduling decisions
that are often required in distributed environments.  Scheduling
decisions can be made on basis of a defined set of policies
(e.g. resource capabilities, data/compute affinities, etc.).

%\begin{itemize}
% \item Multi-level scheduling: In a distributed environment often
%   multiple levels of autonomous schedulers are involved.
% \begin{itemize}
% \item The pilot job is schedules using the local RMS or some other
%   Grid (meta)-scheduler.
% \item UW are scheduled to US on application-level within the
%   pilot-job.
% \end{itemize}
%\end{itemize}

	
	
    % \item \textbf{Agent:} Does every pilot-job implementation have an agent? 
    % What are the tasks of an agent? \alnote{Decision for now: agent is not a 
    % fundamental component. Can we cover this in other components, e.g. discuss 
    % multi-level scheduling in task scheduling.}
    % \begin{itemize}
    %     \item Wikipedia Definition: may refer to one who acts for, or in the 
    %     place of, another, by authority from him; one entrusted with the 
    %     business of another.
    %     \item Difference between implementation and role of agent
    %     \item Agent explicitly in the model? Or requirements that must be met by 
    %     some component in the model?
    %     \item \textbf{Roles of an agent:}
    %     \begin{itemize}
    %         \item manages control flow
    %         \item mapping tasks to resources
    %         \item resource management
    %         \item coordination between different components
    %         \item state management of the complete pilot job state
    %         \item discovering local information
    %         \end{itemize}
    %     
	% \item Infrastructure Independence (Supported middleware types and infrastructures)


% \jhanote{I think upto this point, there should not be mention of
%   BigJob -- we should be talking implementation independent, e.g., must
%   not have sub-job and bigjob agent}


\subsection*{Summary}

% \jhanote{To generalise the previous point, we need to have a few
%   sentences that aims to put all the elements, characteristics
%   together and presents a {\it unified} view of the working of the P-*
%   Model.}
 

\begin{figure}[htbp]
    \centering    
    \includegraphics[width=0.5\textwidth]{figures/pstar_model.pdf}
    \caption{P*-Model: Elements and Interactions}
    \label{fig:figures_pstar}
\end{figure}

Figure~\ref{fig:figures_pstar} illustrates the interactions between the
components of the P* framework. A typical usage mode consists of the following
steps: The application specifies the number of resources required using a UW
description (step 1). During the instantiation of a PJ and the assignment of
resources to the PJ, pilots are queued and started via the local resource
manager (step 1-4). The pilot is then executed on a certain resource (step 5).
Most PJs utilize a Master/Worker coordination model, i.\,e.\ the manager tightly
controls the actions of the worker (the agent). However, it is feasible to
include more logic and decision capabilities into the agent. Generally, the
application assigns a UW to the PJ framework in most cases via the Manager
component, which is then responsible for scheduling the UW. Scheduling has a
spatial component (which US?) and a temporal component (at what time?). To
optimize execution, the Manager can group multiple UWs into a US. In the
simplest case one UW corresponds to one US. The manager then binds a US to
specific PJ according to a specified policy or a user provided scheduler
function (step 6). The US gets executed to a physical resource on which the
pilot is operating (step 7).


\subsection{Pilot-Data: Extension of P* to Dynamic Data}
\label{sec:pilot-data}

% \jhanote{We should purely by analogy go with BigData, no? Need a
%   diagram that talks about TROY = BigJob + BigData + ``??''. Need to
%   define ``??''}

% \jhanote{note: DARE == Dynamic Application Runtime Environment: TROY +
%   MapReduce + Other capabilities}

% \jhanote{Ideally the background/underlying theory of Pilot Data/Store
%   should be presented outside of TROY -- not sure this will be
%   possible, or where, possibly in the \S II? Maybe as an explicit \S
%   II-D, where we say, ``having defined a P-* model, we extend it to
%   Dynamic Data..''}


%\subsubsection{Overview}
% The concept of correlated access originates in
% Filecules~\cite{Doraimani:2008:FGS:1383422.1383429}.

% \begin{figure}[t]
%     \centering
%         \includegraphics[width=0.4\textwidth]{figures/pilotstore.pdf}
%     \caption{Pilot Data and Store Overview}
%     \label{fig:figures_pilotstore}
% \end{figure}

\jhanote{Motivation for PD: We have discussed the importance of DE for
  compute; there exist similar and important considerations for DE for
  data. Analogous challenges of DE for data as DE for Compute, e.g.,
  importance of issues of (late) binding and decision making} 

Dynamic Execution is at least equal important for data-intensive applications:
applications must cope with various challenging issues e.\,g.\ varying data
sources (such as sensors and/or other application components), fluctuating data
rates, optimizations for different queries, data-/compute co-location etc. Thus,
having defined the P* Model, we propose to extend it to dynamic data so as to
facilitate an understanding of DE. This will motivate an analogous abstraction
that we call \emph{Pilot-Data}. Pilot-Data provides late-binding capabilities
for data by separating the allocation of physical storage and application-level
units of data. Further, it provides an abstraction for expressing both data/data and in conjunction with Pilot-Job data/compute affinities.

\subsubsection*{Extension of P*-Model Elements}

% The P* model can in many parts be applied to pilot-data.
The elements defined by P* (in section~\ref{sec:p_star_elements}) can be
extended by the following elements, which are by symmetry, the
elements which can be used to define the Pilot-Data abstraction:
\begin{compactenum}[A.]
\item \textbf{Pilot-Data Framework} facilitates the late-binding between data 
 units that can dynamically associated with a Pilot-Data object, which is 
 associated with a physical resources. 
\item \textbf{Data Unit (DU):} The base unit of data used by the framework,
  e.\,g.\ a data file or chunk. 
\item \textbf{Pilot-Data (PD):} Allows the logical grouping of files
  and the expression of data-data affinities. This collection of files
  can be associated with an extensible set of properties. One of this 
  property is affinity. A PD containing a set of DUs forms the PJ equivalent of  	
  a UW.
\item \textbf{Pilot-Store (PS):} Binds a pilot-data object to a actual
  physical resource. A pilot-store object can function as a
  placeholder object that reserves the space for a pilot-data object. A PS  
  facilitates the late-binding of data and resource and is equivalent to the 
  pilot-job.
\item \textbf{Pilot-Data Manager} is analogous to the PJ-Manager responsible for 
  managing DU, PD and PS elements. 
\end{compactenum}

% \jhanote{Are these the elements of Pilot-Data? If so, please make
% connection of above elements to earlier defined elements of P-* Model
% explicit, e.g. is Pilot-Data = UW, PS = US etc. Either way, needs
% integration with the following elements} 
\alnote{I would equate a PS with a pilot-job (the placeholder). US is
  IMHO more an internal entity not really exposed to the end-user. The
  PD-Manager framework could e.g. use multiple PS to replicate the
  data}

In summary, a PD is a logical container that describes the properties of a group
of files. A PS is a placeholder reserving a certain amount of storage. By
associating a PD to a PS the data is actually moved to the physically location
associated with the PS.  

%\alnote{insert Millau figure}

\subsubsection*{Extension of P*-Model Characteristics}

\jhanote{Compress into a paragraph which extends the Characteristics
  of P*-Model? i.e., Can Data Characteristics, Access Patterns and
  affinity be demoted from elements, and be put under
  characteristics?}

The in the PJ model established characteristics can be used
to describe Pilot-Data: 
\begin{compactitem}
   \item \textbf{Coordination:} Components of the PD framework must be 
   internally coordinated, e.\,g.\ using a master/worker scheme or a more 
   decentralized decision making scheme based on smart agents.
   \item \textbf{Communication:} Common known communication patterns, such as 
    point-to-point and all-to-all, are used for implementing PD. A   
	 critical task is the management of file transfers.
   \item \textbf{Binding} describes the intelligent grouping and assignment of 
   PDs and UWs to facilitate an optimal performance. 
   \item \textbf{Scheduling} is extended to affinities, i.\,e.\ user-defined 
   relationships between PJs and/or PDs, e.\,g.\ data-data  
   affinities exist if different DU must be present at the same compute element, 
   data-compute affinities arise if data and compute must be co-located for a 
   computation, but their current location is different. The decision of where 
   to place the data and compute is made by the scheduler based on the defined 
   policies, affinities, available static and dynamic resource information. 
\end{compactitem}  

Pilot-Job and Pilot-Data encapsulate cross-cutting properties across data and
computation. Both PJ and PD can be used to express different kind of affinities.
The P* implementation will optimize data- and computing according to the defined
affinities and policies. In the following, we discuss further implementation
considerations for P*.




% \subsection*{Not sure what to with this yet}
% 
% In summary, Pilot-Data is a set of abstractions for expressing data localities 
% and affinities. Pilot-Data can be used to create groups of file clustered
% together using a quantifiable property, such as affinity ($\alpha$)
% e.g., $\alpha = 1.0$ would imply that files are always stored
% together.
% 
% 
% Pilot-Data provides a set of basic operations on top of these file
% groups, whilst Pilot Store is a container that represents a logical
% group of physical files that share the same affinity. Pilot-Store
% containers can be used to express data-data affinities. The
% abstraction supports basic management tasks (create, delete, update,
% move, list).
% 
% The Pilot-Data abstraction serves the following needs:
% \begin{itemize}
% \item Reservation of physical disk space: acquisition of data storage
%   (advanced reservation, place holder)
% \item Virtual destination: dynamically mapping of data to pilot
%   stores.
% \item Runtime environment for $\alpha$ based data
% \item Automatic data partitioning and distribution
% \end{itemize}
% 
% \jhanote{This could be eliminated?} While the application-level
% abstraction enables application developers to model data affinities,
% dependencies etc., the runtime framework will be responsible for
% managing data-compute co-allocations, data-transfers, the dynamic
% expansion of storage pools etc.
% 
% 
% \subsubsection*{PD Impl. Cons.}
% 
% 
% \textbf{Data Characteristics:}
%     \begin{itemize}
%     \item Static data refers to data that is infrequently changed and
% does not need to be moved.
%     \item Dynamic data refers to different spatial and temporal
% properties of data:
%     \begin{itemize}
%     	\item Data that is generated or changing at runtime
% (temporal).
%     	\item Data that is in place or needs to be moved (spatial).
% 	\end{itemize}
% 	\item Streaming data
% \end{itemize}
% \textbf{Data Access Patterns: } While the P* is primarily
% concerned with capturing aspects of distributed coordination, this
% elements is extended to include patterns of data access, e.\,g. co-access or gather/scatter that is e.\,g.\ used by MapReduce.

\subsection{Implementation Considerations}

To implement the P* model, there are additional consideration that
must be taken, e.\,g.\ the exposed end-user abstraction and usage
model, the type of communication (API or service) etc.

%In the following we will discuss how we implement
%use the % developed framework
%to develop the
%TROY -- implementation of the P* model.

% \jhanote{I propose we remove ``framework'' ; if anything it should be
%   Model?}
The PJ implemenation must define the usage model, i.\,e.\ how are
resources allocated, UW specified and assigned, as well as the way how
the framework can be accessed, e.\,g.\ via a service or
API. Pilot-Jobs can be run on different types of homogeneous and
heterogeneous resources. Generally, the Local Resource Manager (LRM)
is the gateway to local resources. This can be e.\,g.\ a PBS/Torque or
WMS service. HPC resources are specifically designed for high-end
parallel jobs, while HTC resources are particularly suited for
independent ensemble of tasks. Different applications require a mix of
HTC and HPC, e.\,g.\ when running an ensemble of MPI jobs.  Different
workload characteristics (UW heterogeneities, parallelism, UW
dependencies, etc.) can be supported by a Pilot-Job, e.\,g.\ via
special information collector and scheduler capabilities.

Further, non-functional properties must be considered:
\begin{compactitem}
		\item \textbf{Fault tolerance:} Large, distributed Grids are highly dynamic 
		and inherently prone to failures and thus unreliable. To deal with failures, 
		systems can deploy strategies, such as automatic resubmission etc.
		\item \textbf{Security} describes the security measures taken by the P* 
		implementation, e.\,g.\ the supported authorization, authentication and 
		accounting (AAA) mechanisms (e.\,g.\   GSI, VOMS, MyProxy, etc.). 
		Further, it can be differentiated between single- and multi-user: The 
		former PJ runs under the identity of a single user and is only able to 
		accept jobs from this user, while the latter is able to accept jobs from 
	   different users.		
		% \item \textbf{Resource abstraction} describes the framework that is used for accessing distributed resources (e.\,g.\ SAGA, GANGA, JGlobus etc.).
\end{compactitem}


\section{TROY: A SAGA-based Implementation of the P-* Model}

%Pilot-Abstraction for Dynamic Execution 

% \jhanote{Mark to put in a para about how the API implements different
%  Pilot-Jobs} \msnote{In hindsight not sure why it was at this place,
%  and what you expected but here is an attempt:}

% \jhanote{Need to define and motivate TROY better, i.e. TROY: BigJob +
%   BigData etc.}

\begin{figure}[t]
	\centering
		\includegraphics[width=0.35\textwidth]{figures/pstar_troy.pdf}
	\caption{P* Model and TROY}
	\label{fig:figures_pstar_troy}
\end{figure}

TROY is an implementation of the P* model (section~\ref{sec:pilot-model}) using
SAGA (see Figure~\ref{fig:figures_pstar_troy}). SAGA~\cite{saga_url,saga_gfd90}
provides a simple, POSIX-style API to the most common Grid functions (e.\,g.\
file, job management and distributed coordination) at a sufficiently high-level
of abstraction so as to be independent of the diverse and dynamic Grid
environments. As shown in Figure~\ref{fig:figures_troy} TROY is located on top
of the SAGA API and runtime environment, which is utilized for job management,
data transfer and distributed coordination. The TROY API maintains the SAGA Look
\& Feel and was designed to be similar to SAGA in appearance and philosophy: it
re-uses many of the well defined (and standardized) semantics and syntax of the
File, Job and Advert API. Further, we aimed for a simple-to-use API exposing as
least amount of details necessary. 

TROY supports different usage modes i) it provides stand-alone
Pilot-Job functionalities, ii) it provides a unified API to various
Pilot-Job implementations (e.\,g.\ Condor-G and Diane) and iii)
enables the concurrent usage of multiple Pilot-Job
implementations. Further, we show in section~\ref{sec:bigdata} how
TROY is extended to support dynamic data and affinity-based
scheduling.

% \jhanote{In this paper, we describe the design and implementation of a
%   SAGA-based Pilot-Job, which supports a wide range of application
%   types, and is usable over a broad range of infrastructures, i.e., it
%   is general-purpose and extensible, and as we will argue is also
%   interoperable with Clouds.}
 

\begin{figure}[htbp]
	\centering
		\includegraphics[width=0.45\textwidth]{figures/troy.pdf}
	\caption{TROY Overview}
	\label{fig:figures_troy}
\end{figure}


\subsection{What is Unique about a SAGA-based Implementation of the
  P-* Model?}
\alnote{these points should be addressed in the introduction above}
\begin{itemize}
\item Consistent with based API and job/file/data model
\item thus programmable (eg. affinity-based PJ) and extensible 
\item ....
\item different usage modes: a stand-alone PJ, API access to other PJ,
  concurrent usage with other PJ (as will be demonstrated)
\end{itemize}


\jhanote{(i) Need for interoperable PJ (ii) focus on TROY API as
  common access layer for different PJ, i.e portability (iii) ability
  to use specific/unique functionality is required. Underlying
  argument is that if you need these properties, then you need to do
  it the SAGA way..}

\subsection{BigJob: A SAGA-based Pilot-Job}

\jhanote{MUST provide SAGA URL for updated BigJob API and
  documentation}

% \jhanote{Alternative title: ``BigJob: TROY Pilot-Job'' ?}

% \jhanote{It is CRITICAL to explain why we need to expose the details
%   of multiple atomic BigJobs to the end-user? Remember part of the
%   whole idea of the exercise is, (i) theory: to provide a framework
%   for understanding any differences, (ii) practice: make all these
%   differences go away from the end user!}  \alnote{Since we were not
%   sure about the term ``atomic'', we could also use base bigjob, or
%   core bigjob}


BigJob (BJ) is a pilot-job implementation of the TROY
framework~\cite{bigjob_web}. It supports a wide range of application
types, and is usable over a broad range of infrastructures, i.\,e.\ it
is general-purpose and extensible.

\alnote{This paragraph focuses on TROY abstraction, it refers to BJ as the
implementation. Should we move it on top of the BJ section?} The TROY framework
\jhanote{Is the ``framework'' a reference to BigJob or TROY?}\alnote{TROY}
consists of the API package, the runtime environment and different PJ
implementations referred to as BigJob. The TROY API is based on the artifacts
defined by the P* model \jhanote{Is framework here P-* ?} defined in
section~\ref{sec:pilot-model}. It defines two description classes that extend
the known SAGA Job Description, the PJ description and the UW description. A
TROY manager represents a pool of resources. Resources can be added by
submitting a pilot-job description to the TROY manager using the
\texttt{add\_resource()} method. Subsequently, UW can be assigned to the TROY
manager. At runtime resources can be added and removed at any time.


\begin{figure*}[htbp]
	\centering
		\includegraphics[width=0.7\textwidth]{figures/distributed_pilot_job.pdf}
	\caption{SAGA-based TROY Implementation - BigJob}
	\label{fig:figures_distributed_pilot_job}
\end{figure*}

% General overview of BJ implementations
The BigJob implementations are plugged into the TROY runtime environment via a
thin adaptor layer (see Figure~\ref{fig:figures_distributed_pilot_job}).
BigJob-SAGA~\cite{saga_bigjob_condor_cloud} is a implementation of TROY built on
top of the SAGA API. The framework utilizes a master/worker coordination model:
The BigJob Manager is responsible for the orchestration and scheduling of the
PJs represented by the BigJob Agent. For submission of the pilots, BigJob-SAGA
relies the SAGA Job API and thus, can be used in conjunction with different SAGA
adaptors, e.\,g.\ the Globus, PBS, Amazon Web Service and local adaptor. The
SAGA Advert Service and API are used for communication between manager and
agent. The binding between UW and PJ takes places at submission (late binding). 
In addition to the vanilla BigJob SAGA various other BigJob implementation
exist, e.\,g. there are specific BJ flavors for cloud resources as Amazon EC2
and Microsoft Azure that are capable of managing set of VMs as well as as a BJ
with a Condor-G based backend. The mentioned BJ implementations are referred to 
as core BigJobs. Core BJs are confined to a single resource and don't allow 
neither elasticity nor late-late binding. 

%paragraph on dynamic capabilities
In many scenarios it is beneficial to utilize multiple resources respectively a
dynamic pool of resources, e.\,g.\ to accelerate the time-to-completion or to
provide resilience to resource failures and/or unexpected delays. The TROY API
allows for dynamic resource additions/removals as well as late-late binding. The
support of this feature depends on the backend used. To address the deficit of
the core BJ, the concept of a BigJob pool is introduced; A BigJob pool consists
of multiple core BJs that can be distributed across several resources and types
of infrastructures. Each BigJob manages it's own resources. An extensible
scheduler is used for dispatching UWs to the different BigJobs that are managed
by a BigJob pool (late-late binding). BigJob pools are elastic, i.\,e.\
resources can added or be removed at runtime. Various other backends support
resource elasticity out-of-the-box, e.\,g.\ DIANE and Condor. The BJ pool
framework provides an extensible scheduler. By default a FIFO and an
affinity-aware scheduler are provided.

% For this
% purpose, the dynamic BigJob provides an introspection API that applications can
% utilize to query the manager for a list of current resources.

%  Thus, BigJob-DIANE provides dynamic
% capabilities out-of-the-box, the traditional BigJob
% implementation~\cite{saga_bigjob_condor_cloud} utilizes two layers: the
% \emph{Core BigJob} and the so called \emph{BigJob Pool} layer. Core BigJobs are
% confined to a single resource and represented by a single master process
% (i.\,e.\ a single BigJob Manager). Generally, Core BigJobs are used to
% encapsulate resource specifics, e.\,g.\ for a certain type of infrastructure
% (Condor, Cloud, etc.). A BigJob Pool manages multiple Core BigJobs.

%\jhanote{Should Table I be zapped?} \alnote{gone}  



% \begin{table}[t]
% \centering
% \begin{tabular}{|p{1.8cm}|p{1.7cm}|p{1.7cm}|p{1.7cm}|}
% 	\hline
% 	&\textbf{UW Binding} &\textbf{Coordina\-tion} & \textbf{Communica\-tion} \\
% 	%\hline
% 	%\textbf{BigJob} & &&&&\\
% 	\hline
% 	BigJob-SAGA &&&\\
% 	\hline
% 	\hspace{2mm} Globus/PBS   &at UW submission
% 									  &Master/Worker &SAGA Advert \\  
% 	\hline
% 	\hspace{2mm} Cloud (EC2)  &at UW submission 
% 									  &Master/Worker &SAGA Advert \\ 
%     \hline
%    BigJob-Condor &after UW submission &Master/Worker &Condor-internal \\
% 	\hline
%  	BigJob-Cloud &at UW submission   &Master/Worker 
% 				 &local Python queue / SAGA Job (SSH adaptor) \\ 
% 	\hline
% 	BigJob-Azure &at UW submission
% 	             &Master/Worker &Azure Storage \\ 
% 	\hline
%     BigJob-Diane &after UW submission  &Master/Worker &CORBA  \\ 
% 	\hline	
% 	% \textbf{Dynamic BigJob} & &&&&\\
% 	% 	\hline
% 	%     Dynamic BigJob &late binding (after job submission) &same as BJ &central decision making &same as BJ &(yes in future)\\
% 	%     \hline
%     %   ManyJob-Cloud &late binding (after job submission) &no &central decision making &SAGA Job (SSH) (push) &no\\
%     % \hline 
%     % ManyJob-Affinity &late binding (after job submission)
%     % &yes &central decision making &SAGA Advert (push/pull) &no\\
%     % \hline
% \end{tabular}
% \caption{SAGA-based Troy Implementations: Characteristics According to
%   Defined Vectors} \label{tab:pilotjob_overview}
% \end{table}		


% Aspects that need to be addressed:
% \begin{itemize}
%     \item Big-Job Agent: capacity (physical size) is a property of an agent. 
% 	cardinality: how many sub-jobs can be managed by an agent?
% 	\item Sub-Job Agent: Agent assignment should be separated from resource 
% 	assignment. Agent has the freedom to assign tasks to sub-job in any way 
% 	it want. Agent can do local decisions.    
%   \item Would it make sense to use the ``internal'' versus
%     ``external'' coordination concept to distinguish sub-job
%     versus big-job agent
% \end{itemize}





% Dynamic BigJob provides the ability to dynamically add and remove resources to a 
% big-job. The API consists of two parts, the resource management and the resource 
% introspection part:
% \begin{itemize}
%     \item \texttt{add\_resource()}: New resources are added by starting a new
%     big-job.There are various flavors of this method:
%     \begin{itemize}
%         \item \texttt{add\_resource(re\-sour\-ce\_dic\-tionary)}: Start another big-job on the resource defined in the \texttt{resource\_dictionary}.
%         \item \texttt{add\_resource(affinity, number\_cores)}: Add another big-job to the specified affinity group.
%     \end{itemize}
%     \item \texttt{remove\_resource(bigjob)}: Removes the big-job from the
%     resources.
% \end{itemize}
% 
% Higher-level wrappers that encapsulate e.\,g.\ the specific resource
% descriptions can be implemented. Further, to implement this dynamic resource
% capabilities it is necessary to provide different dynamic resource introspection
% in the dynamic big-job layer:
% \begin{itemize}
%     \item \texttt{get\_resources()}: returns a list of managed big-job objects.
%      Each big-job object can be queried for it's allocated resources (number 
%      nodes, number cores).
% \end{itemize}


% It uses SAGA BigJob approach to start multiple BigJobs agents 
% whether on a single resource or on multiple resources. And 
% these agents are responsible for pulling the tasks from advert 
% service and run the possible subjobs concurrently or in generations.

% \jhanote{The rationale behind dynamic BJ is that for the same
%   application scenario different BJ with different
%   properties/characteristics may be required. Thus dynamic BJ maybe
%   comprised of either homogeneous or heterogeneous atomic BJs}
% \alnote{did one pass... still not perfect}

% \jhanote{I propose the next 3 subsections -- bigjob-cloud, bigjob-aws
%   and bigjob-azure, be removed}\alnote{gone, there was no news anyhow}

% \subsubsection{BigJob for Cloud Computing}
% 
% \jhanote{Once again -- why do differences in execution details between
%   grids and clouds result in the need for different atomic BJs needs
%   to be explained. Must emphasis that the API remains the same.}
% 
% At the execution level, clouds differ from Clusters/Grids in at least a couple
% of different ways. In cloud environments, user-level jobs are not typically
% exposed to a scheduling system; a user-level job consists of requesting the
% instantiation of a virtual machine (VM). Virtual machines are either assigned to
% the user or not (this is an important attribute that provides the illusion of
% infinite resources). The assignment of job to a VM must be done by the user (or
% a middleware layer as BigJob). In contrast, user-level jobs on grids and 
% clusters are exposed to a scheduling system and are
% assigned to execute at a later stage. Also a description of a grid job typically
% contains an explicit description of the workload; in contrast for Clouds a user
% level job usually contains the container (description of the resource
% requested), but does not necessarily include the workload. In other words, the
% physical resources are not provisioned to the workload but are provisioned to
% the container.  Interestingly, at this level of formulation, pilot-jobs attempt 
% to provide a similar model of resource provisioning as clouds natively offer. 
% 
% \subsubsection{BigJob and SAGA AWS Adaptor}
% 
% BigJob provides support for various cloud computing environment. The SAGA BigJob
% implementation can be used in conjunction with the AWS adaptor for SAGA to run
% on EC2-based cloud infrastructures, such as FutureGrid. However, there are some 
% limitations mainly caused by the restrictions of SAGA/AWS adaptor for the SAGA 
% Job package. The SAGA job service object e.\,g.\ does not provide a mean to 
% specify a set of resources. Using the AWS adaptor it is only possible to utilize 
% a single VM instance, which must be configured prior to the run in a 
% configuration file. If multiple VMs are required, the dynamic BigJob 
% implementation must be used. In this case however, it is still not possible to 
% run MPI jobs across multiple VMs. 
% \smnote {why is it not possible to run MPI jobs across Multiple VM's?} \alnote{MPI jobs are (unless you do something outside of BJ) constraint to run
% on resources managed by a single BJ agent. The agent must generate a nodefile
% from this list of resource it is managing. The agent is not aware of resources
% managed by another BJ)}
% 
% \subsubsection{BigJob Cloud \& BigJob Azure}
% 
% To address this limitation, BigJob-Cloud~\cite{saga_bigjob_condor_cloud} was
% developed. BigJob-Cloud provides an implementation of the BigJob API, which is
% completely independent from the SAGA (and thus, the SAGA AWS adaptor). It
% directly utilizes the Amazon tools to access cloud resources. It can manage
% cluster of VM; for this purpose BigJob provides a rich interface for describing
% cloud resources. For this purpose a Python dictionary is used (see
% section~\ref{sec:api}). The VMs can be managed centrally by the BigJob manager:
% All VMs have a public IP and there is no need to interface with a local resource
% manager (SAGA BigJob e.\,g.\ evaluates the \texttt{\$PBS\_NODEFILE} to obtain a
% list of resources). Thus, it is not necessary to deploy an agent on the VM - all
% necessary metadata can be obtained from the AWS backend. Job are spawned via
% SSH.
% 
% % \begin{itemize}
% %   \item ManyJob is required to manage the set of VMs. The BigJob-Cloud can manage a set of VMs without the need of ManyJob.
% %   \item Bigjob uses advert server for communication between BigJob-agent and BigJob whereas BigJob-Cloud does not use an advert server.
% %   \item Bigjob-cloud does not require SAGA-AWS adaptors as opposed to requirement in original Bigjob. 
% % \end{itemize} 
% 
% 
% BigJob-Azure~\cite{10.1109/CloudCom.2010.85} utilizes a similar approach as
% BigJob-Cloud. It utilizes the Azure REST interface to startup VM Worker Roles.
% However, since Azure does not support SSH access it is necessary to utilize an
% agent-based approach. For communication between the agent and the manager the
% Azure Storage is used.
% 
% \msnote{If BigJob is the atomic unit, it should not differ per
%   backend}\alnote{That's mainly a restriction of the job package which
%   does not really map to AWS. There is no common way in the job
%   package to specify the \# of resources that suppose to be
%   used. Thus, this limitation}


\subsection{BigData: A SAGA-based Pilot-Data Implementation}
\label{sec:bigdata}
% \jhanote{theory goes upfront; implementation and architecture stays
%   here} \alnote{ok}

BigData is the SAGA-based implementation of the Pilot-Data abstraction.
Figure~\ref{fig:pilot-data-architecture} gives an overview of the
architecture. The system consists of two components: the PD manager and
the agents deployed on a specific physical storage resource. The coordination 
scheme used is master/worker with some intelligent that is lying de-centrally at 
the BD agent. As communication mechanism the SAGA Advert Service is used.


The PD manager is responsible for 1) meta-data management, i.\,e.\ it keeps
track of the pilot stores that a pilot data object is associated with, 2) it
schedules data movements and data replications taking into account the
application requirements defined via affinities and 3) for managing data
movements. The PD and PJ manager work closely together to manage compute-/data-affinities for applications

% \jhanote{Need to explain/describe architecture of BigData? using the
%   terminology of Section II and P*-Model}

\begin{figure}[htbp]
    \centering
        \includegraphics[width=0.49\textwidth]{figures/pilot-data-manager.pdf}
    \caption{\textbf{Pilot-Data Architecture:} The PD-Manager is exposing the TROY the core of 
    BigData. The application can access the TROY API}
    \label{fig:pilot-data-architecture}
\end{figure}

Each pilot store is managed by an agent. The agent can be started manually or 
using the Pilot Data API. A possible implementation option would be the 
integration of the PD and BigJob agent, which is particularly useful for 
managing data-/compute-affinities.

A core part of the data manager is the data scheduler. The scheduler aims for a
optimum of data and compute locality for an applications: it selects the storage
and compute elements for a UW submitted. For this purpose the PJ scheduler was
extended to support affinities, i.\,e.\ when scheduling UW it considers
dependencies toward PDs. The manager is then responsible for managing necessary
file transfer, the pre-fetching of files if applicable, as well as the executing
the actual UW. For file-transfer management BigData utilizes SAGA and thus, is
infrastructure independent. It supports all underlying SAGA adaptors (SSHFS,
GridFTP) and future adaptors such as Globus Online.



% \subsubsection{BigJob and BigData Integration}
% 
% In particular for data-intensive applications data locality is an important
% concern. Different types of affinity, e.\,g.\ data-data or data-compute, exists.
% Dynamic BigJob provides support for data-compute affinities. Each resource
% (i.\,e.\ each big-job) can be assigned to a certain affinity. The affinity-aware
% scheduler then ensures that sub-jobs that demand a certain affinity are only
% executed on resources that fulfill this constraint.


% \subsubsection{Related Work}
% 
% Work on optimizing file transfers: Kosar[2011]
% Work on reliable file transfer: RFT, Globus Online
% 
% \emph{iRods}
% \emph{Stork}
% 
% 
% \emph{BitDew}
% 
% Random Notes
% \begin{itemize}
% 	\item Focus on Desktop Grid
% 	\item Java-based implementation (ie difficult to interface with Python-based PS/SAGA)
% 	\item highly distributed: stable and volatile nodes
% 	\item pull model, i.e. a node pulls for new data
% \end{itemize}
% 
% 
% Mapping to BitDew:
% \begin{itemize}
% 	\item Pilot Store in its current implementation covers Bitdew Data Catalog and Repository
% 	\item For data management and placement the Active Data API and the Bitdew data scheduler could be used
% 	\item Transfer Management is done via SAGA File API	
% \end{itemize}

% How to evolve pilot data/store?
% \begin{itemize}
% 	\item Active management of data (e.g. replication, automatic affinity management) requires an active component:
% 	\begin{itemize}
% 		\item Manager/Agent model as in BigJob?
% 		\item Who runs active components? Started as part of batch job or separate install/start?
% 	\end{itemize}
% \end{itemize}
% 
% Questions:
% \begin{itemize}
%     \item How should
%     we store data in order to effectively cope with non-uniform demand for
%     data? 
%     \item How many copies of popular data objects do we need? 
%     \item Where should we store them for effective load balancing?
% \end{itemize}

% \textbf{TODO/Future Work}
% 
% 
% Limitations:
% \begin{itemize}
%     \item No active agent that monitors state of files
%     \item No placement policy support or autonomic behavior
%     \item Infrastructures generally expose insufficient locality/topology information
%     \item Compute â€“ Data Affinity: Dynamic BigJob with affinity only provides a very coarse-grained affinity
%     \item No policy for whatâ€™s happening if data is not available in right location:
%     \begin{itemize}
%         \item Run anyways â€“ affinity is just an hint
%     \end{itemize}
%     \item When to move pilot stores? Move or copy?
%     \item Move data to compute or visa versa?
%     \item Data Replication: Identification of the same file: logical filename -> physical files. Manage replication process (consistency!)
% \end{itemize}

\subsection{Conclusion}

In the future we plan to further refine the TROY API, e.\,g.\ by generically supporting different security models via a context API (similar to the SAGA Context API).

The TROY API is available at~\cite{troy_api}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Understanding Other Pilot-Jobs}


\jhanote{Depending upon where the TROY API is discussed we can have
  two ways forward. If TROY API is discussed in \S 3, then we go for
  Mode I, where Mode I: The aim of this section is to show: (i) that
  our P* Model can be used to explain/understand DIANE, (ii) Show that
  the TROY API can be used to marshall Diane stand-alone, (iii) Using
  TROY API, both BigJob and Diane can be used standalone}

\jhanote{If TROY API is discussed in \S 5, then we go for Mode-II,
  where Mode II: The aim of \S 4 is to show: (i) that our P* Model can
  be used to explain/understand DIANE.  Then in \S 5, after having
  discussed TROY API we, (ii) Show that the TROY API can be used to
  marshall Diane stand-alone, (iii) Using TROY API, both BigJob and
  Diane can be used stand alone}

\jhanote{I think there was agreement to go with Mode II}

As more applications take advantage of dynamic execution, the Pilot-Job concept
has grown in popularity and has been extensively researched and implemented for
different usage scenarios and infrastructure. The aim of this section is to show
that our P* Model can be used to explain/understand DIANE as well as other
Pilot-Job frameworks such as Condor-G and Swift.
Table~\ref{table:bigjob-saga-diane} shows how the elements P* model can be
mapped to these frameworks.

\begin{table*}[t]
\centering
\begin{tabular}{|p{2.5cm}|p{3cm}|p{3cm}|p{3cm}|p{3cm}|}
\hline
\textbf{P* Element} &\textbf{BigJob-SAGA} &\textbf{DIANE} &\textbf{Condor} 
&\textbf{Swift Coaster}  \\
\hline
Manager &BigJob Manager & RunMaster & condor\_master, condor\_collector, condor\_negotiator, condor\_schedd &Coaster Service\\ 
\hline
Pilot-Job &BigJob Agent  & Worker Agent &condor\_master, condor\_startd &Coaster Worker\\
\hline
Unit of Work &Sub-Job &Task &Job &Application Interface Function (Swift Script)\\
\hline
Unit of Scheduling &Sub-Job &Task &Job &Job\\
% \hline
% Dynamic Resources &no/yes &yes (AgentFactories)\\
\hline
\end{tabular}
\caption{P* Elements and Pilot-Job Frameworks} \label{table:bigjob-saga-diane}
\end{table*}

\subsection{DIANE}

% Coordination and Communication
DIANE~\cite{Moscicki:908910} is a task coordination framework which implements
the Master/Worker pattern. DIANE was originally designed for master/worker
application, but also provides PJ functionality for job-style executions. By
nature DIANE follows the master-worker coordination model and utilizes a single
hierarchy of worker agents. The manager process is referred to as
\texttt{RunMaster}. Further, there is ongoing work on a multi-master extensions.
For the spawning of PJs a separate script, the so-called submitter script, is
required. Once the worker agents are started they register themselves at the
RunMaster. In contrast to BJ-SAGA, a worker agent generally manages only a
single core and thus, by default is not able to run parallel applications
(e.\,g.\ based on MPI). In contrast to BJ-SAGA, DIANE worker agents possess less
local autonomy and are directly controlled by the master. BJ-SAGA delegates the
management of the local resources from the BJ manager to the BJ agent. The agent
collects information about local resources and manages the allocation of these
resources locally. For communication between the RunMaster and worker
agents point-to-point messaging based on CORBA (omniORB) is used. CORBA is also 
used for file staging, a feature currently not fully supported by BJ-SAGA.

% Binding 
DIANE is primarily designed with respect to HTC environments (such as
EGI~\cite{egi}), i.\,e.\ one PJ consists of a single worker agent with the size
of 1 core. BJ-SAGA in contrast is designed for HPC systems such as TG, where a
job usually allocates multiple nodes and cores. To address this issue a
so-called multinodes submitter script can be used: the scripts starts a defined
number of worker agents on a certain resource. However, UWs will be constrained
to the specific number of cores managed by a worker agent. A flexible allocation
of resource chunks as with BJ-SAGA is not possible. By default a UW is mapped to
a US; application can however implement smarter allocation schemes, e.\,g.\ the
clustering of multiple UWs into a US.

%Scheduling
DIANE includes a simple capability matcher and FIFO-based task scheduler.
Plugins for other workloads, e.\,g.\ DAGs or for data-intensive
application, exists or are under development. The framework is extensible:
applications can implement a custom application-level scheduler.


%Other impl. related issues: FT and security
DIANE is as BJ-SAGA a single-user PJ, i.\,e.\ each PJ is executed with the
privileges of the respective user. Also, only UWs of this respective user can be
executed by DIANE. For communication, DIANE currently relies CORBA/TCP. The 
implementation of GSI is on TCP-level is possible, but currently not yet 
implemented. Further, DIANE supports fault tolerance: basic error detection and propagation mechanisms are in place. Further, an automatic re-execution of UWs is possible.


\subsection{Condor-G}

Condor-G pioneered the Pilot-Job concept~\cite{condor-g}. Using Condor-G 
a complete Condor pool can be initiated using the GRAM service; subsequently 
jobs can be submitted to this pool using the standard Condor tools and APIs.

Condor Flocking lets multiple Condor pools share resources.



% Security
Condor supports different security schemes: for authentication it integrates
both with local account management systems (such as Kerberos) as well as grid
authentication systems such as GIS. Communication traffic can be encrypted.



\subsection{SWIFT and Coaster}

SWIFT~\cite{Wilde2011} relies on its scripting language to describe abstract
workflows and computations. The language provides among many things capabilities
for executing external application as well as the implicit management of data
flows between application tasks. For this purpose, SWIFT formalizes the way that
applications can define data-dependencies. Using so called mappers dependencies
can be easily extended to files or groups of files. The runtime environment
handles the allocation of resources and the spawning of the compute tasks. Both
data- and execution management capabilities are provided via abstract
interfaces. SWIFT supports e.\,g.\ Globus, Condor and PBS resources. The pool of
resources that is used for an application is statically defined in a
configuration file. While this configuration file can refer to highly dynamic
resources (such as OSG resources), there is no possibility to manage this
resource pool programmatically. By default a 1:1 mapping for UW and jobs is
used. However, SWIFT supports the clustering of UWs into a large US as
well as PJs for which the term Coaster~\cite{coasters} is used. Coaster
relies on a master-worker coordination model; communication is implemented using
GSI-secured TCP sockets.



\jhanote{It should probably be Coasters -- which is their notion of a pilot-job.
Just to keep life interesting, they call it head-job and not pilot-job!}
\url{http://www.ci.uchicago.edu/swift/guides/release-0.92/userguide/coasters.php
}


\alnote{SWIFT eval: no standard resource abstraction (SAGA), proprietary 
language (not Python), TODO: check how coasters work! 1 coaster == 1 Condor-G 
job?}


\subsection{Other Pilot-Jobs and Conclusion}

Table~\ref{table:pilot-job-comparison} compares the three PJ frameworks
discussed in this section. TROY enables the interoperability of different PJ
frameworks and promotes easy-of-use.


In addition to the three Pilot-Job framework discussed in this section, various
other frameworks exist.
\begin{itemize}
    \item MyCluster~\cite{1652061} enables the 
	creation of a Condor, PBS or SGE clusters on-demand.
    \item Falkon~\cite{1362680} is a Pilot-Job framework that emphasizes the 
	performance of its task dispatcher.
    \item Nimrod/G~\cite{10.1109/HPC.2000.846563}
    \item DIRAC~\cite{1742-6596-219-6-062049} is another pilot-job framework 
	used by the LHCb community.
    \item ToPoS~\cite{topos} is a REST-based web service primarily designed with 
	respect to parameter sweep applications. Internally, ToPoS utilizes PJ 
	capabilities to efficiently manage resources.
    \item The Production and Distributed Analysis System 
	(PanDA)~\cite{1742-6596-219-6-062041} is the workload management system of 
	the ATLAS experiment. PanDA utilizes multi-user PJs for resource management. 
	The PJ component is built on top of Condor-G and referred to as AutoPilot. 
	It can also be used independently of the ATLAS environment. 	
\end{itemize}


\jhanote{Can we add some structure to these *other* PJ.. this will be
  ambitious and time-consuming, but if we can, that'll be (i) a great
  service to the community, (ii) a strong intellectual addition to the
  paper by virtue of validation of the P*-model} \alnote{which are the minimal P* elements and characteristics we should discuss here?}



\begin{table*}[t]
\centering
\begin{tabular}{|l|p{2.5cm}|p{2.5cm}|p{2.5cm}|p{2.5cm}|}
	\hline
	\textbf{P* Characteristic}
	&\textbf{SAGA BigJob} &\textbf{DIANE} &\textbf{Condor Glide-In} &   
	\textbf{SWIFT Coaster} \\ \hline
End User Environment &API &API and Master/Worker Framework &CLI Tools &Swift script\\ \hline

Coordination &Master/Worker (push) &Master/Worker (pull/push) &Master/Worker &Master/Worker \\ \hline
	
Communication &Advert Service &CORBA &TCP &GSI-enabled TCP \\ \hline

UW Binding &Late/Late-Late &Late-Late &Late-Late &Late-Late\\
% \hline
% MPI/Multinode Applications &yes &no (yes with custom implementation of ApplicationWorker)\\
\hline
UW Scheduling &FIFO, custom &FIFO, custom &Matchmaking, priority-based scheduler 
&Load-aware scheduler, UW clustering\\
\hline

Security &Middleware dependent (GSI, Advert DB Login) &GSI &Multiple (GSI, 
Kerberos) &GSI\\ \hline

Resource Abstraction &SAGA &Ganga/SAGA &Globus &Resource Provider API/Globus CoG 
Kit \\ 
\hline
Agent Submission &API &Ganga Submission Script &Condor CLI 
&Resource Provider API\\
% \hline
% Application Interfaces &Big-Job/Sub-job Management &Big-Job/Sub-job 
% Management\linebreak[4] Master/Worker API (\texttt{ITaskScheduler}, 
% \texttt{IApplicationManager}, \texttt{IApplicationWorker}) &&\\
\hline
Fault Tolerance &Error propagation &Error propagation, Retries &Error propagation, Retries &Error propagation, retries, replication\\
\hline
	
\end{tabular}
\caption{P* Characteristics and Pilot-Job Frameworks}\label{table:pilot-job-comparison}
\end{table*}

\section{Implementations and Experiments}

In this section we discuss the TROY API. Further, (i) we show that the TROY API 
can be used to  marshall DIANE stand-alone as well as (ii) that the TROY API 
can be used stand alone with both BigJob and DIANE concurrently.

Paragraph how TROY and BJ-DIANE work together goes in here...


Figure~\ref{fig:perf_perf-bfast-bj} compares the performance of BJ-DIANE, BJ-SAGA. Further, it shows that both PJ implementations can be used concurrently. This is particularly useful if the time-to-completion can be reduced in cases where resources on another infrastructure are available.

\begin{figure}[t]
	\centering
		\includegraphics[width=0.4\textwidth]{perf/perf-bfast-bj.pdf}
	\caption{Performance BJ-SAGA versus BJ-DIANE}
	\label{fig:perf_perf-bfast-bj}
\end{figure}



Multinode DIANE not used? 1 worker agent always only starts one bfast run, correct?

\note{Call, 26.06.: DIANE must be installed at every run. Setup time vs. startup
time How does this scale with the number of nodes? }

\section{Conclusion and Future Work}


implement a more decentral distributed control mechanism in TROY

\section*{Acknowledgements}
Mark Santcroos is sponsored by the program of BiG Grid, the Dutch e-Science Grid, which is financially supported by the Netherlands Organisation for Scientific Research, NWO.


\bibliographystyle{plain}
\bibliography{pilotjob,saga.bib,saga-related}
\end{document}


\note{Facilities provided include the creation of a PJ, insertion of
  tasks, and attachment to a CPU resource pool for late-binding task
  execution.}  \note{Tasks are ultimately loaded onto specific
  resources using the pilot-job and late-binding. In other words}
\note{PJ provides a mechanism to decouple â€œtask coordinationâ€ from
  â€œresource mappingâ€.}
