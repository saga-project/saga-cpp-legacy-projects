\documentclass[a4paper,10pt]{article}

%\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{url}
\usepackage{float}
\usepackage{times}
\usepackage{multirow}
\usepackage{listings}
\usepackage{times}
\usepackage{paralist}
\usepackage{epsfig}
\usepackage{subfigure}
\usepackage[hypertex]{hyperref}
\usepackage{subfigure}
\usepackage{color}
\usepackage{ifpdf}
\usepackage{wrapfig}

\newcommand{\I}[1]{\textit{#1}}
\newcommand{\B}[1]{\textbf{#1}}
\newcommand{\BI}[1]{\textbf{\textit{#1}}}
\newcommand{\T}[1]{\texttt{#1}}
\newcommand{\dctf}{dC$_{25}$ }
\newcommand{\dctfnsp}{dC$_{25}$}
\newcommand{\atf}{A$_{25}$ }
\newcommand{\dco}{dC$_{1}$ }
\newcommand{\atfnsp}{A$_{25}$}
\newcommand{\dconsp}{dC$_{1}$}
\newcommand{\aonsp}{A$_{1}$}
\newcommand{\ao}{A$_{1}$ }
\newcommand{\ato}{A$_{1}$ }
\newcommand{\ahl}{$\alpha$HL }
\newcommand{\ahlnsp}{$\alpha$HL}
\newcommand{\prim}{$^{\prime}$ }
\newcommand{\primnsp}{$^{\prime}$}
\newcommand{\total}{$0.57M$}
\newcommand{\up}{\vspace*{-1em}}

\pdfpagewidth 8.5in
\pdfpageheight 11in 

\setlength\topmargin{0in}
\setlength\headheight{0in}
\setlength\headsep{0in}
\setlength\textheight{9in}
\setlength\textwidth{6.5in}
\setlength\oddsidemargin{0in}
\setlength\evensidemargin{0in}
\setlength\parindent{0.1in}
\setlength\parskip{0.25em}

\ifpdf
 \DeclareGraphicsExtensions{.pdf, .jpg}
\else
 \DeclareGraphicsExtensions{.eps, .ps}
\fi

\newcommand{\jha}[1]{ {\textcolor{red} { ***Jha: #1 }}}

\begin{document}
\title{\large Scale-Up and Scale-Out of Ensemble-based Simulations}

% Using A Loosely-Coupled Ensemble of Tightly-Coupled Simulations to Understanding the Energetics and Conformational Configurations of Nucleic Acids}

\author{Principal Investigator: Shantenu Jha$^{1,2}$ \\
\small{\emph{$^{1}$Center for Computation \& Technology, Louisiana State University, Baton Rouge, USA}}\\
\small{\emph{$^{2}$Department of Computer Science, Louisiana State
      University, Baton Rouge, USA}}}

\newif\ifdraft
\drafttrue
\ifdraft
\newcommand{\amnote}[1]{ {\textcolor{magenta} { ***AM: #1c }}}
\newcommand{\jhanote}[1]{ {\textcolor{red} { ***SJ: #1 }}}
\newcommand{\michaelnote}[1]{ {\textcolor{blue} { ***MM: #1 }}}
\else
\newcommand{\amnote}[1]{}
\newcommand{\jhanote}[1]{}
\newcommand{\michaelnote}[1]{ {\textcolor{blue} { ***MM: #1 }}}
\fi


\date{28 February 2011}

\maketitle

\subsection*{Summary:} 

This work is built on extensive efforts over the past several years that has produced a wide range of computational science and computer science results. In this request, we propose to use LONI resources, in conjunction with TeraGrid resources to investigate a broad-range of science and computer science problems using loosely-coupled ensemble-based simulations.  We will use multiple LONI \& TeraGrid resources, often concurrently, as well as individual resources to study several scientific problems. We are developing cyberinfrastructure that will enable the collective utilization of TeraGrid resources.  Specifically, in this proposal we request \total SUs for five distinct projects: (1) understanding translocation of nucleic acid in $\alpha$-Hemolysin protein pores; (2) elucidating the underlying mechanism of metabolite binding assisted folding of SAM-I riboswitches and other riboswitches, and, (3) developing and enhancing the understanding of distributed applications and testing the scale-out performance of a range of applications.  % Project 3b will be carried out as part of a collaboration with Prof. Peter Coveney (UCL, Yale);
Projects 3b is part of an international collaboration between TeraGrid and DEISA that is being led the PI (Jha) (involving funding from both the NSF and the EU). The projects for which resources are being requested are all funded projects -- mostly at the National/International level, and some by local resources.  The request for \total SUs in this proposal is based upon the projected science problems as outlined below and a proven track record -- as measured \& evidenced by publications in leading computational science journals, of {\it successfully utilizing} several tens of Millions of SUs in the past three years.  The PI has 13.8M SU hours from TeraGrid/TRAC for the two years (2010-2012).

% We propose to use multiple LONI resources both in concurrent usage mode, as well as individual resources to study several scientific problems.  This work is built on the extensive efforts over the past two years we have carried out with a wide range of computational science and computer science projects. % requiring a few groups to use multiple resources on the TeraGrid concurrently (for loosely-coupled simulations). 
% Specifically, in this proposal we request 0.6M SUs for two distinct Computational Biology projects: (i) Understanding translocation of nucleic acid in $\alpha$-Hemolysin protein pores (ii) Elucidating the conformational switching of S-box riboswitches. Additionally a third Computer Science Projects -- Expeditions in Distributed Computing, is being proposed. The projects for which computer time is being requested are all funded projects -- mostly at the National/International level, and some by local resources. Project 2 which is not currently funded by Federal/National agencies is the basis for upcoming significant proposals.  Additionally, the request for 0.6M SUs in this proposal is based upon the projected science problems as outlined below as well as a proven track record of {\it successfully} utilising more than 1.2M SUs in the last year (large allocation of 900,000 SUs on Queen Bee, several small DAC award and multiple concurrent starter awards).  The PIs group has published several papers acknowledging the use of LONI resources.

% \jhanote{Yaakoub, please determine the correct units?  Is it SUs, NU or some other metric. Thanks.}

% \jhanote{Issues: Final Number of CPU Hrs?  Which resource? What component should be roaming?}
\section{Results From Prior Awards}

This group has multiple publications arising from earlier LONI Awards. Initial results from Project 1 supported by the most recent LONI award to PI Jha was selected as the cover article of a leading scientific journal~\cite{jctc_cover}.  \begin{wrapfigure}{r}{0.28\textwidth}
 \includegraphics[width=0.28\textwidth]{jctc_cover2.png}
 \caption{\footnotesize JCTC Cover Article}
 \label{cover}
\end{wrapfigure}
Using previous TRAC allocations, we have performed all-atom MD simulations for the SAM-I riboswtich study.  We have reported our scientific results in recent conferences~\cite{ecmls10}, as well a recently published journal article in the prestigious Nucleic Acids Research\cite{SAM-I-NAR2009} (Impact Factor $>$ 7.0). On the basis of these publications, we have enhanced our scope by expanding our interests \& simulations to other riboswitches, such as TPP riboswitches and recurrent RNA structural motifs such as K-turn, towards a comprehensive understanding of the mechanisms of SAM riboswitches.  Also, we added the complementary computational approaches such as RNA secondary structure prediction in addition to all-atom MD simulations, and our initial effort was presented in the workshop, "Emerging Computational Methods for the Life Sciences" held in conjunction with ACM HPDC 2010\cite{ecmls10}.

%\input{prior.tex}

We have also used previous LONI-TRAC allocation for CO$_2$ sequestration studies and reservoir characterization studies. These studies were composed of running an ensemble of independent simulations followed by an anaylsis step, and iterating until a termination criteria is met.  Since the simulations are independent, they can be distributed across TeraGrid resources, with a sufficiently abstract and sophisticated workflow manager.  We have begun developing an rudimentary distributed autonomic framework -- Lazarus ~\cite{gmac}, using which we are able to select multiple TeraGrid machines concurrently, dynamically optimize the work-load and gain deep insights. This is a natural continuation of our award winning work in 2008 (TeraGrid 2008 Performance Challenge Award)~\footnote{\url{http://www.cct.lsu.edu/~sjha/select_publications/tg08-performance-award.pdf}}. Beside the obvious science results ~\cite{TG10yye00}, we learned a great deal about scale-out behavior and management of ensembles of large scale simulations, the need for fault tolerance and autonomic behavior, as well as the promising possibilities of combining Grid and Cloud environments. This work has resulted in several publications \cite{Cloud1,Cloud2,MSEScience,TG10yye00} and presentations as well as a Masters degree~\cite{Elkhamra2009}.

\section{Scientific Projects}
\up A common underlying theme of all the scientific projects proposed here, are that they are composed of simulations that involve multiple (loosely-coupled, or uncoupled) ensembles; thus these applications are ideal candidates for the collective usage of the multiple TeraGrid resources for faster time-to-solutions. However, for a myriad set of complex reasons (including misguided NSF oversight of the TeraGrid~\cite{dpa-grid09} application-level capabilities and software environment.), it remains difficult to utilize multiple computational TeraGrid resources concurrently towards the solution of a single problem instance.  Not only will this proposal further the science that can arise by using multiple concurrent resources, this proposal will also further and harden the ability to use multiple resources and make it available to the broader TG user community, i.e., although the focus is domain-specific computational science this project will also address the challenges in effective usage of distributed resources.

\section{Project 1: Nucleic Acid Translocation through Alpha-Hemolysic Protein Nanopore}

\begin{figure}[!h]
  \begin{center}
    \includegraphics[width=5.0in]{ahl_labelled13}
  \end{center}
  \caption{Figure from JCTC Publication, representing the starting configuration of a 3\prim led \atf translocation simulation. The heptameric protein pore \ahl (green) is inserted into a lipid bilayer (black). Features of the translocating molecule include the backbone of \atf (dark yellow) and the nucleic acid bases (blue). For the sake of clarity, water molecules, sodium and chloride ions are not displayed (they are found along the entire length of the pore).}
  \label{fig:edge}
\end{figure} 


With this approach we have been able to explain the observed differences in experimental translocation time through the nanopore between polyadenosine and polydeoxycytidine. Poly(A) and poly(dC) molecules of 100-200 bases in length exhibit a 20-fold difference in translocation time through \ahl in SCCR experiments~\cite{akeson}. The translocation of both 25 base polynucleotides and single nucleotides through $\alpha$-hemolysin has been investigated. An example of our results that qualitatively agree with experimental findings can be seen in Fig.~\ref{full_trans_local}. These simulations are computationally intensive as they employ models with atomistic level resolution; in addition to their size, these systems are challenging to study due to the time-scales of translocation of large asymmetric molecules. Our simulations have provided insight into the role of the interactions between the nucleic acid molecules and the protein-pore. Mutated protein-pores have provided confirmation of residue-specific interactions between nucleotides and the protein-pore. By harnessing such molecular dynamics simulations, we have gained new physical insight into the translocation process.

This work has been published in the {\em Journal of Chemical Theory and Computation}~\cite{jctc_cover}, where we pushed cv-SMD to new limits, testing the validity of the method for a larger translocating molecule and higher atom count system than previously attempted at such relatively low pulling speeds. We showed that an interaction between a positively charged lysine residue of the pore interior and the negatively charged nucleotide phosphate groups give rise to peaks in the free energy profiles. We also highlighted key ion interactions that play a role in these phosphate-lysine interactions, pointing to important considerations for future computational scientists to consider. The work that has been published so far covers relatively low sampled instances of nucleotide and single nucleotide translocation through wild type and mutated protein pores, yielding good results and has established the groundwork for further publications.

\begin{figure}[!h]
  \begin{center}
    \includegraphics[width=4.0in]{full_trans_1every3_b}
  \end{center}
  \caption{Figure from JCTC Publication, where a) Local free energy profiles of \atf and \dctf translocation from the top of the constriction to the bottom of the $trans$-entrance; each profile was derived from two samples. Labeled along the $x$-axis are protein residues Met-147, Lys-113, and Leu-135.  The residue labels span 5~{\AA} from when the pulled atom to first phosphate atom passes the labeled residue. The plot shows poor separation of \atf and \dctf when considering the error bars, though a general trend where \atf has a higher free energy profile than \dctf is observed. An overall increase in the free energy profile is observed from left to right, which is expected as more residues enter the confining dimensions of the constriction and transmembrane barrel. b) Global free energy profile of \atf and \dctf translocation from the top of the constriction to the bottom of the trans-entrance; each profile was derived from two samples. The plot shows good discrimination of \atf and \dctf beyond the error bars. The error bars indicate greater sample-to-sample variation around the constriction and towards the end of the transmembrane barrel, which may indicate non-steric contributions at these points.}
  \label{full_trans_local}
\end{figure}

Using the LONI \& TRAC grant that was awarded to us last year, we have significantly improved the sampling of these results, and amassed further data. For the latest set of data, each profile is calculated from 16 samples (instead of 2-4 samples previously), which has greatly improved the quality and reliability of the data. Systems \dctfnsp-WT, \dconsp-WT, \atfnsp-WT, \aonsp-WT, and \dctfnsp-Mut, were included in the JCTC publication with a low number of samples; these now stand at 16 samples each. Systems \dconsp-Mut, \atfnsp-Mut, and \aonsp-Mut, were not previously explored and now have full 16 sample data-sets. Using these data-sets we have been able to make conclusions such as the negligible impact of the nucleotide base sterics, and are able to point to nucleotide base stacking as one of the causes for the experimentally observed difference between poly(A) and poly(dC) translocation times.

Additionally, over the last year we have used the molecular models established in the cv-SMD simulations to test the system using an alternative translocation method. The method we examined was Adaptive Biasing Force (ABF). Here the translocating atom (and thus the attached molecule) is moved with a biasing force, which acts to overcome energy barriers in order to translocate along a reaction coordinate. The biasing force adapts to the free energy landscape on-the-fly, calculating the free energy and biasing force based on the forces acting on the atom in question, and applying the force directly to that atom. In this way, ABF surpasses the need for certain approximations to be made related to the use of Jarzynski's Equality and the requirement of a stiff cv-SMD harmonic spring, allowing us to assess the impact that these approximations had on the cv-SMD data. Furthermore, ABF does not constrain the biased atom in axes orthogonal to the reaction coordinate, allowing enhanced sampling of the reaction coordinate. So far, we have examined key ABF simulation parameters to allow for the exploration of the \ahl-nucleotide systems, and have produced single sample free energy profiles for \atfnsp-ABF and \dctfnsp-ABF. This work is now being prepared for submission to the Journal of Computational Science (Expected September 2011).



% \begin{table}[!h]
% \begin{center}
%   \caption{The translocation molecules and pore types
% to be simulated. `Wild Type' indicates \ahl with no mutated residues;
% 'Mutant' indicates \ahl mutant L147M.\newline  
% }
% \label{table:systems}
% \begin{tabular}{| c | c | c | c | c | c | c |}
% \hline
% Pulling & System & \ahl Type & Nucleotide & Nucleotides & Samples &
% SUs \\
% Method & Name &  & Base &  & Performed & required \\
% \hline
% cv-SMD & \aonsp-WT & Wild Type & Adenine & 1 & 16/16 & - \\
% cv-SMD & \atfnsp-WT & Wild Type & Adenine & 25 & 16/16 & - \\
% cv-SMD & \dconsp-WT & Wild Type & Deoxycytosine & 1 & 16/16 & - \\
% cv-SMD & \dctfnsp-WT & Wild Type & Deoxycytosine & 25 & 14/16 & 25K\\
% cv-SMD & \aonsp-Mut & Mutant & Adenine & 1 & 2/16 & 150K \\
% cv-SMD & \atfnsp-Mut & Mutant & Adenine & 25 & 2/16 & 200K \\
% cv-SMD & \dconsp-Mut & Mutant & Deoxycytosine & 1 & 2/16 & 150K \\
% cv-SMD & \dctfnsp-Mut & Mutant & Deoxycytosine & 25 & 2/16 & 200K \\
% ABF & \atfnsp-ABF & Wild Type & Adenine & 25 & N/A & 0.25M \\
% ABF & \dctfnsp-ABF & Wild Type & Deoxycytosine & 25 & N/A & 0.25M \\
% \hline
% \end{tabular}
% \end{center}
% \end{table}

\subsubsection*{Requested Computational Resources}
 
For systems of this size (approximately 325000 atoms) and for the timescales of interest, the only option is to use a HPC parallel MD Engine, such as NAMD.  We use NAMD on either 128 or 256 processors typically. The ABF simulations should consume approximately 250,000 (0.25M) SUs in total, though the precise number is difficult to predict due to the nature of ABF.

\section*{Project 2: Computational Study of non-coding Functional RNAs: Binding mechanism of Riboswitches}

%Increasing attention has focused on targeting RNAs for drug design~\cite{foloppe}. 

While increasing evidence indicates a critical role of structured nc-RNAs in gene regulations of both transcription and translation levels, suggesting the potential of targeting RNA for drug design~\cite{foloppe} and bioengineering applications, an understanding of the interactions of nc-RNAs with other molecules such as small metabolite, proteins, and RNAs remains as fundamental challenges.  The roles of computational means, therefore, are critical component for holistic understanding on dynamics and interactions of RNAs.

Among nc-RNAs, our primary targets are riboswitch RNAs. Riboswitches are regulatory RNAs that control the expression of downstream genes. Small metabolite molecules, such as amino acids, nucleotides, coenzymes etc., can bind to riboswitches as effectors in vivo~\cite{mandal}.  In our recent research efforts, the S-box riboswitch (also called SAM-I riboswitch), one member of the riboswitch family that regulates genes related to the metabolism of sulfur and methionine, has been extensively investigated with atomistic simulations.  This riboswitch choose alternative conformation depending on binding of a SAM .  When S-adenosylmethionine (SAM) is bound, the aptamer domain forms anti-anti-terminator (AAT) conformation, which turns off the downstream genes by forming the terminator (T). Otherwise, the anti-terminator (AT) is formed prohibiting the T element formation for continuing transcription process (Figure 3a)~\cite{brooke}. Although the structure of the s-box in the anti-anti-terminator (AAT) conformation has been solved via X-ray crystallography, it is just a static view of how SAM binds to the s-box.  Using extensive all-atom simulations, we became to propose a novel binding mechanism of the SAM-I riboswitch with a SAM in which the role of entropic barrier for the AAT formation as well as the role of $Mg^2+$ specifically bound in the tertiary core structure.  We reported our simulation results in recent conferences and the paper of obtained results were recently submitted to the journal, Nucleic Acids Research.  The paper is now under the review.~\cite{SAM-I-NAR2009} Continuing our efforts, the goal of this study is to probe the dynamic interactions between the s-box riboswitch and SAM at the nanoscale and to explore determinants for the specificity. In particular, we aim to extend our main strategy, combining MD and statistical analysis, for i) other constructs of SAM-I that differ from each other in potentially different secondary structures and tertiary interactions, ii) different sequences in SAM-I family, and iii) other SAM riboswiches (SAM-II and SAM-III) for which X-ray structures were recently reported.  To estimate binding affinity, we employed the Molecular Mechanics - Poisson Boltzmann Surface Area (MM-PBSA) approach and expect to develop novel theoretical developments because of the challenges arising from strong electrostatic interactions involved.  Sampling is a also very important task and for that purpose, replica exchange molecular dynamics (REMD) protocol is attempted.  Our recent development for the distributed adaptive REMD, that is described below, will help us to carry out simulations for these sizable systems.

Also, related experiments are carried out in collaboration with Prof. Fareed Aboul-ela (who has several funded projects -- both Federally and Regional for experiments in collaboration with our simulations). Since a significant advantage of atomistic simulation is the ability to be able to explore details that are typically inaccessible to experiments; while at the same time providing the opportunity for our simulation results will be validated using biochemical and biophysical experiments.

\begin{figure}
\begin{center}
  \subfigure[]{\includegraphics[scale=0.60]{ss-schema}} \hspace{0.05in}
  \subfigure[]{\includegraphics[scale=0.40]{ligand-atom2}}
\end{center}
\caption{(a) Schematic of the secondary structures of s-box riboswitch with SAM bound (left; in the AAT state) and without SAM 
(right; in the AT state), (b) predicted ligand-SAM interactions with s-box}
\end{figure}


\begin{figure}
\begin{center}
  \subfigure[]{\includegraphics[scale=0.33]{sbox_3D}}
  \subfigure[]{\includegraphics[scale=0.33]{binding_pocket-2}}
  \subfigure[]{\includegraphics[scale=0.33]{binding_pocket-1}}
\end{center}
\caption{(i) The structure of the S-box riboswitch; (ii) S-box with SAM bound; (iii) Residue number: 7, 11, 
12, 45-47, 57-59, 88, 89}
\end{figure}


\subsubsection*{Initial Results}

Here, we brief MD simulation protocols used for the long time MD simulations of the SAM-I riboswitch and new findings reported in the paper~\cite{SAM-I-NAR2009}.  The starting point of all simulations is the X-ray crystal structure of SAM binding to the AAT conformation of s-box riboswitch (PDB: 2GIS)~\cite{montange}. In the simulation of the SAM free s-box riboswitch, SAM is directly removed from the x-ray crystal structure and replaced with solvent water. The amber99bsc0 correction force field is used here~\cite{alberto}. Parameters for SAM are from the Generalized Amber Force Field (GAFF) and missing parameters are calculated using ANTECHAMBER~\cite{wang}. Positions of added hydrogens are guessed using PSFGEN within NAMD 2.6. Then the RNA molecules are solvated in a cubic solvent box of TIP3P waters with a 16A padding in all directions. Sodium and magnesium ions are distributed around the RNA molecules and neutralize charge of the system. The total number of atoms in the system is 56,000. Energy minimizations are carried out on all of the systems to remove bad contacts. Starting from 0 K, the temperature is raised 10 K for every 10,000 steps and is held constant after reaching the desired temperature (310 K) using temperature reassignment. MD simulations are performed in the NPT ensemble with the pressure maintained using the Langevin piston method with a period of 100 fs and decay times of 50 fs. The time step is 2fs for both equilibration and production phase. Bond lengths between hydrogens and heavy atoms are constrained using SHAKE. The long-range electrostatics is treated with the Particle Mesh Ewald (PME) method with a cutoff distance 12A.  All MD simulations are carried out by using a parallel version of NAMD 2.6.  VMD, wordom~\cite{moe} and homemade scripts are employed to analyze the trajectories. All snapshots of structural images are made using VMD.

%All simulations are performed using NAMD 2.6 on LSU (Tezpur) and LONI (Queenbee) Linux clusters. 



\begin{figure}
\begin{center}
   \subfigure[]{\includegraphics[scale=0.42]{rmsd}} \newline
   \subfigure[]{\includegraphics[scale=0.49]{RMSD_residue}} \end{center} \caption{RMSD of overall s-box and binding pocket only in SAM bound and WOSAM (short for the trajectory of SAM free s-box riboswitch) trajectories with reference to the crystal structure; (b) Root mean square fluctuation (RMSF) and B-factor of each residue of s-box riboswitch from MD trajectories} \end{figure}

\begin{figure}
\begin{center}
   \subfigure[]{\includegraphics[scale=0.45]{cluster_2D}}
   \subfigure[]{\includegraphics[scale=0.35]{cluster_1D}} \end{center} \caption{Clustering and Principal Component Analysis (PCA) point towards a chopstick-like motion involving P1 and P3 helices in the absence of SAM.  (a) Projections of snapshots of the SAM free trajectory are plotted against the first two principal components and color coded according to a k=3 k-means clustering: cluster 0: magenta, cluster 1, green, cluster 2, magenta. Representative snapshots from each cluster are also shown. This plot indicates that snapshots can be broadly clustered into two groups (cluster 1 and cluster 3) with cluster 2 representing a group with characteristics similar to those of cluster 3. The projection along PC1 broadly separates the clusters, while projection along PC2 completes the separation between clusters 1 and 3. Structures of representative snapshots indicate that clusters are distinguished by a dramatic change in relative position of P1 and P3. (b) From top to bottom: The time evolution of the first principle component of the SAM free trajectory. RMSD for each snapshot in the SAM free relative to the representative snapshots for cluster 1 (cyan curve) and for cluster 3 (magenta curve). The distance between the Center of Mass (COM) of P1 and P3 for the SAM free trajectory (blue) and for the SAM bound trajectory (red).  During the first half of the SAM free trajectory, P1 and P3 helices move apart (clusters 0 and 2), then they move back together during the second half of the trajectory (cluster 1).}

\end{figure}

Our initial achievements are illustrated in Figure 5 and Figure 6.  Our results suggest that the presence of SAM in the binding pocket is critical to form P1 helix overcoming the entropic cost for bringing two distal strands in proximity.  The essential dynamics found with the SAM-free trajectory are shown in Figure 6 with clustering results, indicating the characteristic long time dynamics in the SAM-free system. 

To estimate the binding affinity, we chose MM-PBSA and according to our results, strong electrostatic interactions between SAM and SAM-I as well as the importance of inner-shell bound $Mg^{2+}$ binding are well noticed as critical components for the overall SAM binding mechanism.  

\begin{figure}
\begin{center}
  \includegraphics[scale=0.660]{56k_scaling-2} \caption{Wall-clock times taken (in second) for each step at different processor counts. The measurement was done with Queen Bee }
\end{center}
\end{figure}

% \begin{table}[h]
% \begin{center}
%   \caption{Riboswitch simulations and expected computing resources. Note that the estimated SUs for MM-PBSA calculation is about the additional MD simulation with the SAM-free RNA system}
% \label{table:systems}
% \begin{tabular}{| c | c | c | c |}
% \hline
% Molecular System & Type of Calculation &   Method or Package  &   SUs required \\
% \hline
% SAM-I & MD &  NAMD &  30K\\
% SAM-I & MM-PBSA & NAMD & 30K \\
% SAM-I & PCA/Clustering &  AMBER and scripts &  50K\\
% SAM-II &MD &  NAMD &  15K\\
% SAM-II & MM-PBSA & NAMD & 15K \\
% SAM-II & PCA/Clustering & AMBER and scripts & 50K \\
% SAM-II & REMD &  NAMD &  450K\\
% SAM-III &MD &  NAMD &  30K\\
% SAM-III & MM-PBSA & NAMD & 30K \\
% SAM-III & PCA/Clustering & AMBER and scripts & 50K \\
% \hline
% \end{tabular}
% \end{center}
% \end{table}

\subsubsection*{Requested Computational Resources}

% We estimated the total computational time based on our simple benchmark and the benchmark found in the TeraGrid web site as mentioned above. 
In Figure 7, we show the scaling performance with MD simulation package such as NAMD.  The benchmark was done with Queen Bee.  When using 32 cores, the time taken per step is approximately 0.06s; thus the wall clock time required to complete 1ns is .34 day; in other words for a 56K system, 1 ns simulations require $\approx$ 300 CPU hours.  Thus each 100 ns simulation requires approximately 30,000 CPU hrs, based upon which, we expect 120,000 SUs are needed for this project.

% and considering similar performance between Ranger and Kraken, (also assuming Queen Bee's performance close to Abe. See {\url{http://staging.teragrid.org/userinfo/aus/namd_benchmark.php}} ) 500,000 SUs are requested with Ranger and the remaining 250,000 SUs are requested with Kraken, respectively.


\section*{Project 3: Expeditions in Distributed Computing using SAGA}

\subsubsection*{3a: Developing and Deploying Applications using SAGA}

A wide range of applications have been developed using SAGA -- ranging from regular compute intensive applications but involving multiple resources ~\cite{saga_escience07, gmac, REMD-PhilTranA2009}, applications with multiple components and possibly irregular runtime requirements~\cite{saga_loosely_coupled, teragrid08} as well data-intensive applications ~\cite{saga_data_intensive, saga_grid_cloud} using programming abstractions such as MapReduce. %~\footnote{Implementation of MapReduce using SAGA is funded by Google} 
An initial prototype of a ``general pilot-job'' framework using SAGA that can be utilized for Replica-Exchange that enables the {\it trivial} utilisation of multiple distributed resources across distributed infrastructure has been developed. This is currently work in progress, but we anticipate sufficient progress to begin testing the framework using our 56K riboswitch model.~\cite{REMD-PhilTranA2009}. A Brief schematic of Distributed Adaptive Replica Exchange Molecular Dynamics is shown in Figure 8.  Our current framework implements the Generalized Pilot-Job feature with the BigJob abstraction built upon SAGA and utilization of the framework is the key component for successful massive REMD simulations for our project on riboswitch studies.


\begin{figure} \begin{center} \includegraphics[scale=0.55]{DARE-MD} \end{center} \caption{Schematic of Distributed Adaptive Replica Exchange framework using the BigJob abstraction that is built upon SAGA. It is proposed that DARE-MD framework will ultimately become part of the GridChem Science Gateway (in which the PI and co-PI Kim are involved).} \label{fig:results} \end{figure}

\begin{figure}
\begin{center}
\includegraphics[scale=0.33]{gmaps_bqp.jpg}
\end{center}
\caption{A snapshot of an application using batch-queue-prediction system to dynamically determine the best resource to spawn a sub-task to; the noteworthy point is that the entire decision process is at the application level -- the fact that the application has to spawn a job of requirements X is mapped to a resource requirements, BQP is used to determine the resource based upon optimal availability and then the application uses SAGA to spawn and launch the sub-task onto the chose resource. A paper demonstrating this feature working across the TeraGrid won the Performance Challenge Award at TeraGrid 2008 (Ref.~\cite{teragrid08})}
\label{}
\end{figure}

\subsubsection*{3b: Utilizing Application-level Interoperability across LONI, TeraGrid and DEISA}

As part of NSF funded HPCOPS Award (till July 2011), we are leading a project to utilize the aggregated computational power of the Federated Grids of DEISA and TeraGrid. The aim of the project is to (i) work towards an integrated infrastructure that supports application level-interoperability and, (ii) having created the infrastructure, apply it to the dual challenge of understanding the conformational changes and determining the free energy of biological systems. Specifically, the high-level aim of this project is to enable scientific applications to utilise the federated capabilities of the TeraGrid, DEISA and LONI systems, to enhance the understanding of HIV-1 enzymes and epidermal growth factor receptors (EGFR) implicated in lung cancer -- important science drivers which form the basis of Project 3. The Computer Science aims of this project are to use several Replica-based and Replica-Exchange simulations for HIV-1 \& EGFR research, on multiple TeraGrid, LONI and DEISA resources, working concurrently towards the solution of a single problem instance -- the rapid computation of free-energies of binding with high-levels precision.

{\it Resources Requested:} For Project 3, we require 200,000 SUs. This is a somewhat experimental component of this proposal, where we do not have clear estimates and benchmarking data. However, based upon our experience/publications alluded to in the previous paragraph, each "experiment" that we conduct, (i.e. application that we develop), requires a minimum of 50,000 SUs. The TeraGrid-DEISA Interoperability project -- built upon validated and ready to run models, has already acquired significant resources (several million CPU hours) on DEISA machines; our request for 200K is to support initial production runs of the VPH (Virtual Physiological Human) models and test the infrastructure for Scale-Out tests -- intra-TeraGrid as well as TeraGrid-DEISA Grids, performed on the VPH Models.

% As part of NSF funded HPCOPS Award, LONI has begun a year long project to utilize the aggregated computational power of the Federated Grids of LONI, TeraGrid and DEISA. The aim of the project is to (i) work towards an integrated infrastructure that supports application level-interoperability and, (ii) having created the infrastructure, apply it to the dual challenge of understanding the conformational changes and determining the free energy of biological systems. Specifically, the high-level aim of this project is to enable scientific applications to utilise the federated capabilities of the TeraGrid, DEISA and LONI systems, to enhance the understanding of HIV-1 enzymes and epidermal growth factor receptors (EGFR) implicated in lung cancer -- important science drivers in general. The aim of this project is to use several Replica-based and Replica-Exchange simulations for HIV-1 \& EGFR research, on multiple TeraGrid, LONI and DEISA resources, working concurrently towards the solution of a single problem instance -- the rapid computation of free-energies of binding with high-levels precision.

% {\it Resource Requirements:} For Project 3, we require 100,000 SUs. This in turn is divided into a component for the Interoperability Project (50K) and other SAGA
% (e.g., Sequestration) Projects (50K). The latter is, a somewhat more experimental component of this proposal, where we do not have clear estimates and benchmarking data. However, based upon our experience/publications alluded to in the previous paragraph, each "experiment" that we conduct, (i.e. application that we develop), requires a minimum of 50000 SUs. 

\section*{Summary}
In summary, our request is for a total of \total hours on Queen Bee and other LONI Dell Clusters.  We have published upwards of a dozen papers in the past two years where LONI resources were acknowledged. These can be found at: \url{http://cct.lsu.edu/~sjha/select_publications}. We would like to request an immediate advance allocation of 100,000 CPU-hours pending full proposal review and subsequent aware to avoid disruption in current work.

\begin{table}[!h]
\begin{center}
  \caption{Resource distribution requests for different projects\newline}
\label{table:systems}
\begin{tabular}{|c | c | }
\hline 
Project & Total Request \\ 
\hline
I  & 250K \\
\hline
II & 120K \\
\hline
IIIa & 100K \\
IIIb & 100K \\
\hline
\end{tabular}
\end{center}
\end{table}


% \section*{Supporting Grants}
% The PI leads Work Package 4 of the NSF Funded Cybertools Project (http://www.cybertools.org) (NSF Award Number-00000, Total Value \$12M).  PI-Jha is also the co-PI of LSU/LONI's HPCOPS NSF award, ``Joining the TeraGrid'', member of the Scientific Board of the LONI Institute.  Project 2 is funded by multiple Louisiana Board of Regents award and an LSU Faculty Award (PI Jha). Integration of SAGA with applications is part of Cybertools and the PI also holds multiple peer-reviewed awards for the development and integration of SAGA.  The Interoperability Project~\cite{interop_url} is currently funded by an NSF HPCOPS award, and is being executed by the PI (Jha). See PI's vitae for full grant listing. Part of EnKF work is funded by the UCoMS project, Department of Energy and Louisiana Board of Regents award No. DE-FG02- 04ER46136

% \section*{TeraGrid User Support} Since our applications have been developed, tested and benchmarked on the TeraGrid resources we are requesting allocations on, we do not foresee any need for support from TeraGrid personnel. However, as part of the TeraGrid-DEISA Interoperability, it is probable we will submit a supplemental request for ASTA support (in Q4 of 2009).

\bibliographystyle{unsrt}

\bibliography{jha_loni_alloc_jul01,ucl_trac,yye00}

%\bibliography{jha_loni_alloc_jul01}

%\subsection*{Project 1: Nucleic Acid Translocation through Alpha-Hemolysic Protein Nanopore}

%Translocation of nucleic acid strands through confined protein pores has biological relevance in instances 
%such as viral transfer of genetic material across a membrane.
%Nanopore current recording experiments can reveal information on a polymer translocating through a pore 
%such as the well characterised protein pore $\alpha$-Hemolysin (Fig.1). A nanopore is inserted into a lipid 
%bilayer and a transmembrane potential is applied. The ionic current through the pore is measured and the 
%translocation of polymers through the pore causes a measurable current blockade by obstructing the flow 
%of electrolytes through the pore.
%With high enough output resolution, the sequence of a DNA strand could be attained. By simulating the 
%translocation event, our understanding of the microscopic processes involved will increase. This could 
%move the field towards genetic sequencing.

%The high-level scientific objective for this project is to compute the free energy
%profile of the translocation process of DNA along the vertical axis of a a
%transmembrane protein pore buried in a lipid membrane bilayer. This is a project that has been ongoing for nearly two years (which was 
%initiated before my transition to LSU),  has consumed a total of $\approx$ 1.5M SUs
%so far (of which 200,000 are on LONI) and we are now very close to 
%major publication in a high-profile journal (Fall 2008). 
%Over the past 24 months, we have been performing an extensive series of
%simulations of DNA translocation through the $\alpha$-hemolysin nanopore by a
%novel combination of algorithm and computational infrastructure. The possible
%parameter space of the problem that needs to be explored is huge, but through
%our studies we have
%determined the optimal values at which to perform the non-equilibrium simulations.
%%simulations. This work follows as a natural extension of the work done as part
%%of the SPICE project which won the HPC Analytics Challenge Award at
%%SuperComputing'05 and the Life Science Award at International Supercomputing
%%Conference 2006. 
%The project uses the parallel MD code NAMD and as also led to a 
%grid-enabled version of NAMD developed by us to perform steered MD simulations
%including the capability to connect to distributed haptic devices. This work is currently 
%funded by the UK's EPSRC (equivalent to the US NSF).
%Current and near-future
%work, also forms the basis of a joint US-UK submission to the NSF (in collaboration 
%with Prof.'s Zuzanna Siwy (UC-Irvine) and Stefan Howorka (London)). 

%{\bf Principal scientific objectives:} To compute the free
%energy profile of the translocation process of DNA along the vertical axis of
%a transmembrane protein pore buried in a lipid membrane bilayer.  There are
%presently established methods of experimentally deducing the base sequence of
%nucleic acid chains, however, such methods are not economically accessible
%enough to allow for the decoding of DNA for purposes such as in medical
%analysis. As Akeson~\cite{akeson} have shown, threading nucleic acid chains through
%transmembrane porous proteins (such as $\alpha$-hemolysin) in a single channel
%current recording apparatus can provide information indicating sequence. At
%the resolution of the data retrieved from the experiments of Akeson,
%conclusions were limited, not allowing the differentiation of individual
%bases. It is proposed that steps towards individual base coding by this method
%can be made by increasing the translocation time of nucleic acid chains
%through the pore, or making the extent of blockage more distinct based on the
%type of base. This can be achieved either by chemical modification of the pore
%surface or modification of the nucleic acid chain itself, yielding a higher
%resolution current readout. Experimentally, Akeson showed that threading
%single-stranded dA and dC based nucleic acid chains results in different
%translocation times and different degrees of pore blocking. To understand
%experimental results and explore alterations to this method, computer
%simulations can be performed that compute the free energy profile of the
%various ssDNA sequences that translocate and thus provide the required insight
%into the physical mechanisms responsible for the significant change in the
%translocation time. Using NAMD our simulations  mimic - in a 'natural'
%environment - the translocation process. 

%We use constant velocity Steered Molecular Dynamics (cv-SMD) is used to reduce simulation computation to practical levels. In cv-SMD, fast translocation is performed by pulling the front most residue of a single stranded DNA strand at constant velocity.  The equilibrium free energy profile is extracted from the non-equilibrium work done using Jarzynskiï¿½s Equality~\cite{jarz}.
%In addition to verifying the experimental results (which should be amenable to
%relatively small scale simulations), moving 20-base long single-stranded A and
%dC nucleic acid chains inside the protein pore and analysing differences in
%the free energy profile/work done in the process, will test the ability of our
%method to distinguish difference sequences.
%The ability to compute via simulations, the 
%free energy profiles along sub-sections of the pore has immediate consequences for
%DNA sequencing.

%%Following that we would also
%%like to simulate the movements of longer chains of 50-base length (of both dA
%%and dC) in order to explore the effect that the length of the DNA strand has
%%on the profile(s).

%\begin{figure}
%\begin{center}
%\includegraphics[scale=0.5]{pore_dna-1.jpg}
%\end{center}
%\caption{Single stranded DNA translocation through $\alpha$-Hemolysin. A cross-section of the protein 
%pore $\alpha$-Hemolysin is depicted in blue. Red represents a single strand of DNA (3 end at the bottom).  
%Black represents the lipid bilayer.  The DNA strand is translocted from the cis (top) entrance to the trans 
%(bottom) entrance.}
%\label{}
%\end{figure}

%\begin{figure}
%  \subfigure[]{\includegraphics[scale=0.750]{pore_structure-1}}  \hspace{1.5in}
%  \subfigure[]{\includegraphics[scale=0.750]{pore_structure-2}}
%\caption{These simulations are concerned with translocation at the constriction (blue) and beta barrel 
%(green) of $\alpha$-HL. Red represents the alpha chamber.
%Here the translocation resistance is highest due to the pore dimensions.
%Key pore residues present themselves when pulling poly-Adenine across this region at 0.004 A/ps. 
%Methionine residue 113 (red) and Leucine residue 135 (blue) show peaks in work profile.}
% \label{model}
%\end{figure}

%\begin{figure}
%  \subfigure[]{\includegraphics[scale=0.4]{single_sample_A_vs_dC_135}}  
%  \subfigure[]{\includegraphics[scale=0.4]{dC_vs_A_PMF}}
%\caption{Simulation results show the difference in the work profile of translocation between nucleotide
%chains with differing secondary structure (A vs dC). The figure to the left is for a single sample -- local
%energetic profiles, which is capable of distinguishing the main features and events contributing to the profile.
%The figure to the left is the total work profile over the complete pore, averaged over several samples (six in 
%this case). There are indications that the total translocation work energy distinction between A and dC lies 
%well beyond the error bars.}
% \label{model}
%\end{figure}

%

%\begin{figure}
%\begin{center}
%\includegraphics[scale=0.60]{polyA_polydC.jpg}
%\end{center}
%\caption{Single molecule current traces for poly Adenine and poly-deoxy-Cytosine2.
%Poly-A takes ~20 times longer to translocate than poly-dC.
%; experimental evidence of different translocation times for oligo-nucleotide chains
%of A and dC (differing in secondary structure); the precise mechanism of this
%difference is not understood by experiments; our computer simulations provide
%inside in to the different "dwell" times.}
%\label{}
%\end{figure}

%
%{\bf Resources Requested:}

%The code we use is NAMD - a well established, community code that has been
%extensively validated and tested for scalability (for more
%benchmark/performance data, see:
%http://www.ks.uiuc.edu/Research/namd/performance.html). Timings for our system
%(approximately 320,000 atoms) as benchmarked are: \\
%%1.30 days/ns on 64 cores, 0.60 days/ns on 128 cores, 0.35 days/ns on 256 cores.
%Back when queen bee was running slower~\footnote{we helped diagnose
%a hardware problem with Queen Bee} and we were using 384 processors: \\ % \newline
%  \indent performance on 384 CPUs 0.0529952 s/step (0.306685 days/ns) \newline
%Now it's faster and we are using 256 processors the performance is: \newline
%  \indent on 256 CPUs : 0.0506383 s/step (0.293045 days/ns)  \footnote{with similar performance on Tezpur}

%Thus, simulations of our systems are most efficient on 256 cores (consistent with the NAMD rule-of-thumb of approximately 1000 atoms per core), and it
%takes approximately 2000 CPU hrs to simulate one nanosecond. 
%Thus, at a optimal pulling speed, (as there is a compromise between statistical and systematic errors),  it takes the equivalent of $\approx$ 5ns of simulation time for the ssDNA to be pulled through the relevant and interesting portion of the pore. Given that these are non-equilibrium
%simulations, based upon our experience from the SPICE project~\footnote{see http://www.realitygrid.org/Spice and publications therein}, we need 
%on average 8-12 simulations to get a measure of the free energy profile.  Taking
%10 as the typical number of samples required, we need 10 x 5 x 2000 = 100000 CPU hrs, to
%gather enough statistics for a single  translocation study (for example, a
%specific nucleotide sequence and length).
%A single translocation study represents just the beginning of many simulations that will be needed to explore
%the (rich) phase-space of this problem in order to understand the physical
%properties of this important system and the specific science problems.
%Specifically, the science problems that we are addressing are:
%\begin{itemize}
%\item Effect of secondary structure on the translocation times
%\item  Exploring specific interactions that are the dominant component
%of the "dwell" time within the pore
%\item Understanding the difference between the translocation properties
%of a single nucleotide versus a 20-mer oligo-nucleotide, so as to 
%be able to discern the "single" compoent in the "extended, collective"
%dynamics of longer polymer chains
%\item Mutation studies need to be performed to help distinguish steric interactions from other (e.g.,
%hydrophobic) interactions
%\end{itemize}

%The fundamental computational challenge arises from the requirement to perform
%a multiple number (between 8-12) of samples of the same "pulling event"; this is
%a consequence of the non-equilibrium simulations/methods and thus the stochastic nature of the simulations. However, in spite of this, the method used 
%is still at least an order of magnitude quicker than "slow motion" real dynamics MD simulations.

%We anticipate being able to make progress on items 1 (and related to it item 2)
%and item 5; this is what we anticipate we will be able to achieve over the next six months. 
%For each item, we anticipate a lower bound of 200,000 SUs; thus we are requesting
%400,000 SU hours for project I. We will be back 
%with a request for more hours if we were to effectively study the different specific
%science objectives.



\end{document}



