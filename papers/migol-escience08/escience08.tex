\documentclass[times, 10pt,twocolumn]{article} 
\usepackage{latex8}
\usepackage{times}

%\documentstyle[times,art10,twocolumn,latex8]{article}

%------------------------------------------------------------------------- 
% take the % away on next line to produce the final camera-ready version 
\pagestyle{empty}

\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{url}
\usepackage{float}
\usepackage{times}    
\usepackage{multirow}    
\usepackage{listings}   
\usepackage{times}     
\usepackage{paralist}    

\usepackage{listings}
\usepackage{keyval}  
\usepackage{color}
\definecolor{listinggray}{gray}{0.95}
\definecolor{darkgray}{gray}{0.7}
\definecolor{commentgreen}{rgb}{0, 0.4, 0}
\definecolor{darkblue}{rgb}{0, 0, 0.4}
\definecolor{middleblue}{rgb}{0, 0, 0.7}
\definecolor{darkred}{rgb}{0.4, 0, 0}
\definecolor{brown}{rgb}{0.5, 0.5, 0}

\lstdefinestyle{myListing}{
  frame=single,   
  backgroundcolor=\color{listinggray},  
  %float=t,
  language=C,       
  basicstyle=\ttfamily \footnotesize,
  breakautoindent=true,
  breaklines=true
  tabsize=2,
  captionpos=b,  
  %numbers=left, 
  %numberstyle=\tiny
}      

\lstdefinestyle{myPythonListing}{
  frame=single,   
  backgroundcolor=\color{listinggray},  
  %float=t,
  language=Python,       
  basicstyle=\ttfamily \footnotesize,
  breakautoindent=true,
  breaklines=true
  tabsize=2,
  captionpos=b,  
  %numbers=left, 
  %numberstyle=\tiny
}

\title{Replica-Exchange Simulations on Production Distributed
  Environments using SAGA and Migol}
%\title{Reliable Replica-Exchange Simulations of Biomolecular Systems
%  on Production Distributed Environments using SAGA-CPR and Migol}
%  Computational Grids using SAGA-CPR and Migol}

\author{
  Andr\'e Luckow$^{1}$, Shantenu Jha$^{2,3}$, Joohyun Kim$^{2}$, Andre Merzky$^{2}$ and Bettina Schnor$^{1}$\\
  \small{\emph{$^{1}$Institute of Computer Science, Potsdam University, Germany}}\\
  \small{\emph{$^{2}$Center for Computation \& Technology, Louisiana State University, USA}}\\
  \small{\emph{$^{3}$Department of Computer Science, Louisiana State University, USA}}\\
}

%\date{}

\def\acknowledgementname{Acknowledgements}
\newenvironment{acknowledgement}%
{\section*{\acknowledgementname}%
\parindent=0pt%
}

\bibliographystyle{plain}

\newif\ifdraft
\drafttrue
\ifdraft
\newcommand{\kimnote}[1]{ {\textcolor{green} { ***JK: #1 }}}
\newcommand{\alnote}[1]{ {\textcolor{blue} { ***AL: #1 }}}
\newcommand{\amnote}[1]{ {\textcolor{magenta} { ***AM: #1 }}}
\newcommand{\jhanote}[1]{ {\textcolor{red} { ***SJ: #1 }}}
\else
\newcommand{\kimnote}[1]{}
\newcommand{\alnote}[1]{}
\newcommand{\amnote}[1]{}
\newcommand{\jhanote}[1]{}
\fi

\begin{document} 


\maketitle    

\begin{abstract}
  There exist a class of scientific applications for which utilizing
  distributed resources is critical for a reduced time-to-solution for
  the problem at hand. In addition to the ability to orchestrate the
  many distributed jobs, a major challenge however, in dynamic
  distributed environments with thousands of machines connected to
  each other is fault tolerance. The more resources and components,
  that are involved, i.e., degrees-of-freedom, the more complicated
  and error-prone the system becomes.  We discuss a specific class of
  applications -- Replica Exchange simulations of biomolecular systems
  % example of an application -- Replica Exchange simulations of biomolecular
  % systems -- 
  -- where utilizing as many (often heterogenous) distributed
  resources as possible, is critical for the effective solution of the
  scientific problem, and thus a mechanism to co-exist and handle the
  fault-tolerance inherent in dynamic distributed systems is
  required. In this paper, we discuss the design, development and
  deployment of an unique framework to support fault-tolerant
  distributed simulations; critically our framework is scalable,
  general purpose and extensible. Our framework has two primary
  components: SAGA and Migol.  SAGA is a high-level programmatic
  abstraction layer that provides an interface for the major
  distributed functionality required for application development; we
  provide details of a newly developed functionality in SAGA -- that
  of Checkpoint and Recovery. Migol is an adaptive Grid middleware,
  which addresses the fault tolerance of Grid applications and
  services by providing the capability to recover applications from
  checkpoint files transparently.  In addition to describing the
  design of the SAGA CPR package, the integration of the CPR adaptor
  with the Migol infrastructure, we outline our experiences with
  running a large scale, general-purpose, SAGA-CPR based
  Replica-Exchange application in a production distributed
  environment.

  % In addition to providing details of our implementation, we discuss
  % why our approach is unique.  SAGA~\cite{SAGA_Goodale06a} provides
  % a standardized, application-level API for common Grid scenarios,
  % e.\,g.\ the management of files and jobs. The new SAGA Checkpoint
  % Recovery (CPR) package addresses checkpointing and the automatic
  % recovery of Grid applications. In this paper, we describe the
  % design of the SAGA CPR package, the integration of the CPR adaptor
  % with the Migol infrastructure, and our experiences with running a
  % large scale SAGA CPR based task farming application in a real Grid
  % environment.
    
  % The aim of this paper is, i) to couple the SAGA and Migol
  % frameworks, ii) to provide a proof of concept implementation of
  % the replica exchange method that uses the SAGA/Migol frameworks,
  % iii) to show the validity, scalability and extensibility of this
  % approach by attempting the simulation of a large model system.
\end{abstract}

\Section{Introduction}
                           
Several classes of applications, which are well suited for loosely
coupled Grids, exist; arguably the best known and most commonly used
ones are task farminga pplications. Despite the simple nature, many
problems can be solved with such a model, e.\,g.\ parameter studies
found in many sciences or Monte Carlo simulations. A slightly
morecomplicated, but possibly more interesting than {\it pleasingly
  distributed} applications, is the class of applications that are
with a small level of coupling between the tasks.  
%still essentially loosely-coupled, but 
A common and interesting example that belongs to this category are
\emph{Replica Exchange }~\cite{hansmann,Sugita:1999rm} simulations.
Such simulations are used to understand important physical phenomena
-- ranging from protein folding dynamics to binding affinity
calculations in the computational drug discovery.  At one level, RE
simulations can be thought of as characterised by two distinct
components: the simulation types and the coupling-mechanism between
the individual replicas. Given that there are not only different
simulation strategies -- Monte Carlo or Molecular Dynamics -- but also
multiple levels-of-coupling, it is important to note that RE is in
fact a class of algorithms and not a specific
algorithm~\cite{dpa_surveypaper}.  Based upon the degree (frequency)
of coupling (exchange), we classify RE as either regular or irregular
exchange. Well known examples of the former
are~\cite{hansmann,Sugita:1999rm}; we classify parallel replica
dynamics in the latter. Parallel Replica dynamics~\cite{SPdynamics,
  pande_bj03} involves coordination amongst the replicas (implemented
either using the master-worker or the peer-to-peer) at irregular
intervals -- say everytime there is a folding event, whereas in the
former category there is an attempt to exchange states between certain
pairs at fixed intervals.

% Grid computing envisions the sharing of compute, network, data and
% software resources across multiinstitutional virtual organizations
% (VOs).

% Replica Exchange~\ref{repex1, repex2}
% simulations %reference does not exist in bib
% are a class of algorithms that belong to this category.  Algorithms
% that can effectively utilize distributed computing infrastructures
% are often different to those that utilize canonical high end
% computers.  Replica exchange (RE) methods~\cite{SPdynamics,
%   pande_bj03} are a class of algorithms that can utilize distributed
% computing infrastructure
  
%\kimnote{note:  previously: `` conformational configuration changes.'')}

% \jhanote{I may be responsible for this imbalance in emphasis, but I
%   think we need to add at least one powerful paragraph outlining why
%   fault tolerance and recovery is critical in distributed systems. I
%   know we have Section 3 (Migol: Fault Tolerant Service FW), but we
%   need something more general before talking specifically about Migol
%   as an implementation of those needs. I am sure Andre L has a
%   paragraph on this already from earlier text/papers on Migol. I will
%   then use that to phrase the specific requirements in the context of
%   REMD} 
              

A distributed REMD simulation must be able to orchestrate different
heterogeneous resources in a complex and dynamic environment.  Writing
applications that need to orchestrate heterogeneous resources across
distributed environments is a complex task -- for a myriad number of
reasons, not least of which is that large scale Grids are inherently
prone to failures and thus unreliable.  Additionally, Grids are
geographically distributed and involve many heterogeneous components
as well as multiple organizations.  Several studies indicated severe
failure rates of Grid nodes as well as of Grid
jobs~\cite{schroeder,10.1109/E-SCIENCE.2006.93,DBLP:conf/grid/KhaliliHOSC06}.
Although these analyses have been conducted in different environments
with different hardware and software configurations and measurement
methods, it is obvious that reliability is not an intrinsic property
of Grid environments. Some applications can respond to such failures
via redundant or speculative computing.  However, the redundant
computing approach has its limitations, especially when there is a
level of coupling between tasks.  Speculative computing is still
possible, but its use to mitigate the consequence of distributed
failures, leads to a whole host of load-balancing and scheduling
problems.  As established, distributed RE simulations are
loosely-coupled, hence failure can become a major problem for such
applications.  For example, in a regular exchange simulation the
termination of all replica processes is required before the next
iteration step can be started \jhanote{Jha needs to strengthen/refine
  previous sentence}. That means that a single failed process can halt
the complete simulation.

In replica exchange simulations, there is a need to occasionally
attempt an exchange between pairs of replicas; the pairing of replicas
is also not a constant and is dynamically determined. However, once
the pair of replicas has been established, a delay or loss of one
replica will stall the other replica. In the worst case, a single
failing task can render the entire computation worthless.
\emph{Migol}~\cite{schnorLuckow08} provides a fault tolerant Grid
middleware, which supports the discovery and allocation of resources
as well as the transparent starting, monitoring and recovery of tasks
in the Grid.  Thus, it is essential to intrinsically provide support
for application fault tolerance within the Grid middleware.
\jhanote{there is repetition here. need to refine}

The \emph{Simple API for Grid Applications (SAGA)}~\cite{saga_gfd90}
provides a straightforward API for developing a broad range of
applications, including but not limited to loosely-coupled data and/or
task-parallel applications.
% The framework provides an standardized, easy-to-use 
% API for applications to efficiently utilize Grid infrastructures. 
SAGA offers a standardized, easy to use API for utilizing Grid
infrastructures. The API supports e.\,g.\ the management of Grid jobs,
file transfers and replicas. Thus, SAGA is ideally suited to encode
the logic of orchestration of the replica exchange simulations across
distributed infrastructure.

Having motivated the need for a fault-tolerant framework for
application development, this paper describes the design of the
Checkpoint\,\&\,Recovery package (CPR) of the SAGA API, and its
implementation using the Migol adaptor. SAGA-CPR represents the first
extension to the core SAGA specification, and is thus a validation of
the extensibility of the specification.  It is critical that all
aspects of the distributed application life-cycle be made easier:
design, development and deployment. Thus we will also present our
experiences developing and deploying an REMD application in a
production environment.

We will discuss the main advantages of our approach: i) scalable to
resources of different sizes, as opposed to folding@home which is
BOINC and thus size-limited; ii) as opposed to WISDOM that is confined
to gLITE/EGEE and is usable over varied distributed environments such
as the TeraGrid, DEISA, etc; iii) extensible -- we can implement
Replica-Exchange simulations, but we can also do other applications
with this approach. We need to point that the ability to do all of the
above arises from the use of the correct programmatic and system-level
abstractions: enable users do what users do best and must do i.e.,
provide the simulation and orchestration logic, and let systems and
middleware do what is best left to them, i.e., checkpoint management
\jhanote{a stronger example would be welcome}.

% \jhanote{The important message, which we need to repeat and repeat yet
%   again: our approach is i) scalable over different distributed
%   environments of different size (not like folding@home); ii) general
%   -- usable over TG, DEISA, etc. unlike the WISDOM approach which is
%   tied to EGEE; iii) extensible -- we can do Replica Exchange but we
%   can also do other applications with this approach. We need to point
%   that the ability to do all of the above arises from the use of the
%   correct programmatic abstractions; let the systems/middleware do
%   what middleware/systems do best (Migol); let users do what users do
%   best!}

\jhanote{We need to address: ``Why SAGA and Migol?'' here in the
  opening section}

\Section{REMD Simulation with NAMD}   

In Molecular Dynamics approaches, a sufficient sampling of
configurations constitutes the rudimentary requirement for connecting
atomistic results to macroscopic or thermodynamic quantities available
from experiments.  However, with the most powerful computing resources
at the moment, it is still formidable to carry out the long time
simulations providing the desired conformational search.  There has
been a great number of research efforts to accelerate sampling, and
the generalized ensemble approaches have been proposed to overcome an
inefficient conformational sampling presumably associated with kinetic
trappings.  REMD was introduced by Sugita and Okamoto as one of the
generalized ensemble approaches and they employed Molecular Dynamics
as the name represents.  The REMD run is composed of many replicas
running at different temperatures and at the given time interval,
exchange of parameters such as temperature are determined by the
metropolis scheme.  Various topics have been studied with this
approach successfully, but the size of simulations, requiring many
replicas, could be one obstacle for routine usages.  

\Section{Migol: A Fault Tolerant Service Framework}     
\label{sec:migol}

\begin{figure*}[t]
            \centering
                \includegraphics[width=0.7\textwidth]{migol_architecture}
            \caption{Migol Architecture: Migol provides several services for supporting the lifecycle of Grid applications. The infrastructure transparently handles issues, such as resource, metadata management and application fault tolerance.  Applications that are managed by Migol are transparently monitored and recovered in case of a failure.}
            \label{fig:migol_architecture} 
\end{figure*}           


Migol guarantees the correct and reliable exe\-cution of applications or tasks even in
the presence of  failures. The framework is based on the Globus Toolkit 4. 
Figure~\ref{fig:migol_architecture} shows the current Migol architecture and 
the interactions between the different services.

The fundamental metadata model of Migol is the \emph{Grid Service Object (GSO)} schema,
which defines a generic and extensible information model for
describing Grid applications.  
%When an application is
%started, a Grid Service Object (GSO), which describedis created. 
A GSO stores all relevant information about an application: resource requirements,
the location of binaries and checkpoint files, global unique identifier (GUID),
etc.

Grid Service Objects for all running applications are stored in 
the {\em Application Information Service (AIS)}. 
Applications can register and update service metadata, 
such as files, machines etc.\ through Grid Services Objects. 
To avoid a single point of failure, the AIS is replicated using a ring-based
replication protocol, which ensures  the consistency of the stored data
(see~\cite{Luckow:2008ys} for more details).
%TODO plugin arch erl√§utern

% Figure~\ref{fig:migol_architecture} shows the current Migol architecture and
% the interactions between the different services.  
Applications are started using the {\em Job Broker Service (JBS)} (message 1 in 
Figure~\ref{fig:migol_architecture}). Before a job
submission, the JBS must register the GSO of the application at the AIS (message 2).
Resource discovery is performed through the WS MDS~\cite{schopf06},
which aggregates data of different services, e.\,g.\ the Network
Weather Service (NWS)~\cite{NWS99} (message 3).  Available resources are matched
by the JBS according to the requirements of the application. To better plan the execution of 
an application the JBS supports advance reservation. If the respective Grid environment is capable of handling reservations, the JBS negotiates 
a time slot with all suitable resources. All reservation offers are ranked based on a shortest expected
delay strategy~\cite{Jeske:2007wj}.  For spawning of jobs on the selected resource, the Globus GRAM service is used (message 4).
%For conducting advance reservations and for job executions the
%\emph{Advance Reservation Service (ARS)} is used (message 4).
             
% To ensure the availability of the application state even in case of a failure, 
% checkpoints can be replicated to another site  using the \emph{Checkpoint Replication Service
% (CRS)}.  The CRS offers features such as the automatic discovery and selection of storage resources, 
% the management of the data transfer and the detection of new file versions. 
% The CRS uses an adaptive strategy to determine replication  targets respectively to select a replica for a
% job restart.

Migol provides several mechanisms for supporting the fault tolerance of Grid applications.
% Fault tolerance requires at least two basic mechanisms: failure
% detection and recovery. 
To detect failures, the \emph{Monitoring and Restart Service (MRS)}
periodically checks all services registered at the AIS using an
application-level monitoring mechanism (message 6). In case the MRS discovers an inactive
application due to a timeout, it initiates a restart respectively a migration using the
Job Broker Service (message 7).  

For recovery, Migol relies on application-level checkpointing, i.\,e.\ applications have to be
written to accommodate checkpointing and restart. 
Each time a checkpoint is witten, the application has to update
its Grid Service Object at the AIS (message 5). 

To support an automatic recovery, Grid applications must be able to interact with the Migol infrastructure:
\begin{compactitem}
    \item The application must register checkpoint and job metadata with the infrastructure.
    \item The infrastructure must be able to externally monitor the application.
    %\item The infrastructure must be able to externally notify the application and trigger the saving of a checkpoint.
\end{compactitem}  
The Simple API for Grid Applications (SAGA) provides a feature rich programing abstractions. The described functions 
can be  provided via the new SAGA Checkpoint Recovery API. In addition, SAGA provides  
various other APIs for supporting file transfer, RPC communication etc., which Grid applications can benefit of.


\Section{SAGA Checkpoint Recovery API}

The SAGA API specification in~\cite{saga_gfd90} defines not only the
core of the SAGA API itself, but also the mechanism to extend that API
by additional functional packages.  The SAGA CPR API
Package~\cite{saga_cpr_draft} is such an API package, and is at the
moment an OGF working draft.

The SAGA CPR package provides a clean abstraction for starting,
monitoring and recovering of checkpoint-restartable jobs.
% The package was inspired by the GridCPR~\cite{gridcpr} architecture and Migol.
Using SAGA CPR, applications can register checkpoint and job metadata with the infrastructure. 
Further, the API provides support for starting applications, triggering of checkpoints and recoveries.

For the management of checkpoint-able Grid jobs, SAGA defines the \texttt{cpr::job} and \texttt{cpr::service} class. The handling of 
CPR jobs is similar to regular jobs: A job is defined using a job description. In contrast to normal jobs, 
CPR jobs require  two job descriptions -- one for starting and another one for restarting the application.
Jobs are started using the job service. In addition to the normal job controls, a CPR job can be queried for checkpoint metadata, and 
it can be explicitly checkpointed or recovered. Listing~\ref{lst:saga_job_start} shows the starting of a CPR enabled job.

\begin{lstlisting}[style=myListing, caption={SAGA CPR: Starting a Job}, float=t, label={lst:saga_job_start}]
saga::cpr::service js; 
saga::cpr::description jd_start;
saga::cpr::description jd_restart;
// fill out job description
...
// submit job  
saga::cpr::job job = js.create_job (jd_start, 
                                    jd_restart);
job.run ();
\end{lstlisting}

Listing~\ref{lst:saga_init_service} shows how the application itself can connect to the CPR infrastructure. During instantiation of the \texttt{saga::cpr::service} object the adaptor is able to register itself with the CPR backend. This step can e.\,g.\ be used to register a monitoring endpoint. Using the \texttt{saga::cpr::self} object an application can obtain metadata about the current job from the application.                                                                               
\begin{lstlisting}[style=myListing, caption={SAGA CPR: Initialize Migol Session}, float=t, label={lst:saga_init_service}]
saga::cpr::service service (saga::url 
    ("migol://gridhub.cct.lsu.edu:8443/wsrf/services/migol/AIS-JGroups"));
saga::cpr::self = service.get_self ();
\end{lstlisting}

Further, applications can use a checkpointing API to update checkpoint metadata at the backend. The SAGA CPR API allows the hierarchically 
organization of checkpoint files and directories. Checkpoint directories are 
used to group checkpoints. For example, parallel applications often write one checkpoint per 
processor, i.\,e.\ in case of $n$ processors a CPR checkpoint
would consists of $n$ files. Thus, a CPR checkpoint is designed as container 
for multiple physical or logical files. Listing~\ref{lst:saga_chkpt_reg} demonstrates 
how an application task registers a single checkpoint file with the CPR implementations.     
\begin{lstlisting}[style=myListing, caption={SAGA CPR: Register Checkpoint with Migol}, float=t, label={lst:saga_chkpt_reg}]
saga::cpr::checkpoint remd_chkpt("remd_chkpt");
remd_checkpoint.add_file (saga::url 
  ("gsiftp://qb.loni.org/work/remd/chkpt.dat"));
\end{lstlisting}


CPR jobs are subject to an extended state model. An application can queries or subscribe to a job state via the \texttt{cpr::job} object.      
Figure~\ref{fig:cpr-statemodel} summarizes the CPR state model. The CPR model extends the SAGA job state model by the new state \texttt{recovering}. This state
is used to indicate to the application that the infrastructure is currently trying to restart a job. 
If these recovery attempts fail, the state of the application is permanently set to \texttt{failed}. 
The application must then deploy an application-level recovery schema in order to continue execution. 

\begin{figure}[th]
    \centering
        \includegraphics[width=0.48\textwidth]{cpr-statemodel.pdf}
    \caption{SAGA CPR State Model: The CPR state model introduces the recovering state. If the CPR infrastructure attempts to recover an application, this state is used to indicate to the application that an automatic recovery procedure is conducted.}
    \label{fig:cpr-statemodel}
\end{figure}
 

The Migol adaptor provides a compliant SAGA CPR stack for the C++ reference implementation~\cite{Kaiser:2006qp}. 
A major building block is the application-level monitoring mechanism used to detect failures. 
To support the monitoring arbitrary SAGA applications, a monitoring Web service is started 
by the Migol adaptor with the initialization of a \texttt{cpr::service} object. This Web 
service is implemented using the gSoap HTTP server~\cite{gsoap}.  This service is used by the Monitoring Restart Service,
which periodically sends keep-alive messages to applications.                                                                             
% The URL of the service is registered at the Application Information Service where it can be obtained 
% by the monitoring service.

% A major building block for fault tolerant system is failure detection. In general, fault detection is done by a monitor process, 
% which periodically sends keep-alive messages to applications. To monitor applications, the application must provide a communication endpoint.


Another critical aspects is the management of the application's metadata. To ensure the recoverability 
of an application, metadata, such as the job description and information about written checkpoint files 
must be available even if the application failed. The Migol adaptor relies on the Application 
Information Service (AIS) as metadata backend. Every job is associated with a global unique identifier (GUID).
%, which is initially created when an application first registers. 
All metadata belonging to an application can be stored or queried with reference to the GUID. 
The following information is propagated to the AIS by the Migol adaptor:
\begin{compactitem}
    \item The job description for starting and restarting of an application is mapped to the resource, service, and file profile of the Grid Service Object schema used by Migol. The registration of these information is done during the \texttt{create\_job()} operation.
    \item The SAGA job state is mapped to the more comprehensive Migol model, which introduces additional states such as \emph{migrating} or \emph{pending}. All state transitions are directly propagated to the AIS. State queries, e.\,g.\ using the \texttt{get\_state()} operation,  are always conducted against the AIS. The Migol state is accessible via the \texttt{state\_detail} metric of the job object.
    
    \item After startup of the application, the monitoring endpoint, i.\,e.\ the URL of the embedded SOAP server, is updated.
    \item During the runtime, metadata about checkpoint are periodically updated via the \texttt{cpr::checkpoint} object.
\end{compactitem}
To ensure the availability of these information in case of a failure, the AIS is actively replicated across multiple Grid nodes.
                  



                                                  
% MPI monitoring cannot be done without touch NAMD application
% In the following different implementation issues regarding the Migol SAGA adaptor are discussed.


\Section{Implementing Replica Exchange Using SAGA}

\jhanote{Joohyun: Put in a paragraph or two of the main points of the
  how it is done here; maybe even discussing the ideas related to
  checkpointing, new jobs, MPI issues discussed etc. etc. Try to
  establish what is specific to the SAGA way of doing things from what
  is a general distributed computing problem/issue}

\begin{figure*}[htbp]
      \centering
          \includegraphics[width=0.6\textwidth]{REMDgManager-architecture.pdf}
      \caption{Components of REMDgManager: The main part of the framework is the replica manager. The manager orchestrates a set of replica processes using the SAGA/CPR API. The Migol infrastructure ensures that the REMDgManager and all replica processes are monitored and recovered if necessary.}
      \label{fig:REMDgManager-architecture}
\end{figure*}



% This paper aims to demonstrate the efficient implementation armed with
% grid-enabled SAGA framework as well as fault-recovery SAGA CPR Migol
% infrastructurure.
Our development comprises three components. The first component is a
task manager sitting on a user's desktop and providing a user
interface to manage overall REMD run, the second component is Migol
server that submits, monitor, and recover each replica simulation if
exists, and the final one is task agents that resides on High
Performance Machines where MD simulations are carried out.  The role
of a task manager varies for achieving the best solution with
different local environment at each HPC machine.  A highly scalable MD
package, NAMD~\cite{Phillips:2005gd} is used for a parallel MD
simulation corresponding each replica run.  A REMD task manager and
various parts gluing different components are mostly written with
python scripts since SAGA-Python binding transparently exposes SAGA
and SAGA-CPR/MIGOL APIs to simple but versatile Python.

The REMDgManager provides a scalable infrastructure for running
replica exchange simulations.  The application utilizes the SAGA CPR
API and the underlying Migol infrastructure to efficiently and
reliable manage replica
processes. Figure~\ref{fig:REMDgManager-architecture} illustrates the
architecture of the REMDgManager. The replica manager is the main
component of the framework -- it is responsible for orchestration of
all replica processes, i.\,e.\ the parameterization of replica tasks,
file staging, job spawning and the conduction of the replica exchange
itself. Listing~\ref{lst:python_saga_chkpt_reg} illustrates the
application sequence of a REMD simulation.
 
At each replica exchange step, the REMDgManagers starts a defined
number of replicas. The current parameterization of each replica is
stored within a list object and can be referenced by the
\texttt{replica\_id}. For each replica different metadata, such as the
working directory, the used configuration file, the temperature
etc. is maintained.  Based on this information the REMDgManager
transfers all required files to the respective resource using the SAGA
File API and the GridFTP adaptor (step 1). The replica process is the
started using the CPR API and Migol/GRAM (step 2a-2c).  The job
description of each task is stored within the Migol infrastructure to
ensure a later recovery.  The job start itself is done via the GRAM
service.
  

To integrate NAMD with the SAGA/Migol infrastructure a SAGA based
wrapper application is used.  This wrappers is responsible for
updating the metadata of the application, i.\,e.\ the state,
monitoring endpoint and the URLs of newly written checkpoints at the
Migol backend.  The wrapper then launches the actual NAMD job using
MPI. During the entire runtime the replica process is monitored by the
Migol infrastructure using the monitoring endpoint of the NAMD
launcher. This wrapper enables the flexible orchestration of multiple
NAMD jobs through the REMDgManager without modification of the NAMD
source itself. After all replicas have been successfully completed,
the exchange step is conducted, i.\,e.\ the REMDgManager determines
new parameters for all replica tasks, stages the required files and
relaunches all jobs.

% SAGA APIs and SAGA CPR Migol infrastructure allow a user to build
% his/her own implementation of REMD adaptively considering the
% limitations of available resources.  That aspects comes from the
% high-level abstraction of SAGA and flexible implementation of
% Replica Exchange as well as fault-recovery with SAGA CPR Migol.  For
% example, one simple scenario is to assign a REMD manager in charge
% of a job submission/monitor/fault-recovery.  Each job corresponding
% one replica run with a assigned temperature is sent to HPC machine
% as a parallel MPI job by the manager and overall job execution and
% fault-recovery are monitored by the manager.  The following snippet
% shows the part of the code written with python for the REMD manager
% showing creation and submission of a job corresponding to each
% replica.
                                             
% \jhanote{I say we put a few snippets of code here}
\begin{lstlisting}[style=myPythonListing, float=t, caption={REMDgManager: Replica Orchestration}, 
label={lst:python_saga_chkpt_reg}]

# loop n replica exchange steps  
for i in range(0, n):     
    # spawn jobs
    for replica_id in range(0, num_replicas):
      js = saga.cpr.service()
      jd = create_job_description(replica_id, replica_info)    
      replica_job = js.create_job(jd_start, jd_restart)
      replica_job.run()
      replica_list.append(replica_job)
     
      # wait for termination of jobs
      for replica_id in range(0, num_replicas):     
        replica_list[replica_id].get_state()
        if (state=="Done"):                          
            #obtain result of replica process
            energy[replica_id] = get_energy(replica_id)
 
      # reparameterize jobs before spawning new tasks      
      ...                    
\end{lstlisting}

% Joohyun's old code
% while 1:
%     js = saga.cpr.service() 
%     jd_start = saga.cpr.description()
%     jd_restart = saga.cpr.description()
%     # job description for each replica stored in the configuration file is retrieved
% ...
%    #checkpoint file registration
%    saga.cpr.checkpoint remd_chkpt("remd_chkpt");
%    remd_checkpoint.add_file (saga.url ("gsiftp://qb.loni.org/work/remd/chkpt.dat"));
% ...   
%    #job submit
%     new_job = js.create_job(jd_start, jd_restart)
%     new_job.run()
%     REMDjobs.append(new_job)

 \kimnote{However, this simple scenario faces high overhead since each
   job is submitted to the local scheduler in many cases. \it
   Shantenu, may be you can write here another scenario we discussed
   as an alternative to the simple scenario we are testing.  If I can,
   I will try, too }


\Section{Experiences with REMD}
\label{sec:exp}       
        
To evaluate the performance of the REMDgManager several experiments
have been conducted within the LONI Grid~\cite{Allen:2003xy}. The
REMDgManager has been been deployed on the three Linux clusters of the
LONI Grid: QueenBee, Eric and Poseidon.  QueenBee is the largest LONI
machine and has a peak performance of 50 TFlops.
Figure~\ref{fig:saga-taskfarming} gives an overview about the
REMDgManager scenario used for our experiments.

\begin{figure*}[t]
    \centering
        \includegraphics[width=0.8\textwidth]{saga-taskfarming}
        \caption{Fault Tolerant Molecular Dynamic Simulation: The
          REMDgManager orchestrates a set of distributed replica
          processes using the SAGA API. All processes synchronize
          important metadata with the Migol infrastructure. Migol then
          actively monitors all processes and ensures that, even in
          the presence of failures, all task are eventually
          completed.}
    \label{fig:saga-taskfarming}
\end{figure*} 

It is important to understand the overhead caused by the fault
tolerance mechanisms. This overhead is caused by the necessary
interactions with the Migol backend, in particular for job submission,
registration of metadata and active monitoring.
\begin{figure}[ht]
    \centering
        \includegraphics[width=0.45\textwidth]{performance/perf_submission.pdf}
    \caption{SAGA/CPR Migol Adaptor Overhead}
    \label{fig:performance_perf_submission}
\end{figure}           
Figure~\ref{fig:performance_perf_submission} shows the runtimes of
important CPR API operations in comparison to their non fault tolerant
counterparts. A particular critical operations for REMDgManager is the
spawning of remote tasks. Depending on the number of replica
processes, for each replica step n replica processes must be launched
and monitored.  To evaluate the overhead, a single NAMD task was
started using SAGA CPR/Migol.  The experiment showed that a CPU job
submission is per average 2\,second slower than a normal GRAM
submission. This overhead is attributed mainly to the additional
metadata registration operation required by the Migol.

In addition to the submission overhead, the Migol adaptor shows some
additional initialization overhead. The overall runtime of this simple
REMD submission task including the initialization is with 6.45\,sec
about 4.5\,sec slower than the GRAM launcher. In particular, the
initialization of a Migol jobs requires the setup of the HTTP server
as well as several metadata updates at the AIS, which accounts to the
additional costs.
                                                                                                                     
In addition to the job submission, it is in particular interesting how
the runtime of the replica process is effected by the active
monitoring and the replication of the checkpoints.  To evaluate this,
a NAMD job with 1000 steps was started with and without Migol support.
During this run a very eager monitoring intervall of 1 minute was
used. Checkpoint metadata has been updated every five minutes - a
single checkpoint update required per average 1.3\,seconds per average
as shown in Figure~~\ref{fig:performance_perf_submission}.  During
this 21\,minute NAMD run the overhead average to 20\,s and thus only
amounts to about 2\%.

In the final scenario the REMDgManager has been setup to run a REMD
simulation with 16 replica process and 100 replica exchange
steps. Table~\ref{tab:app_stats} summarizes some application
characteristics.
\begin{table}        
    \centering
	\begin{tabular}{|p{5cm}|l|}
          \hline
          Molecular Dynamics Code &NAMD\\ \hline
          Number of MD steps between exchanges &1000\\ \hline 
          Required staging files/size &5 files/17 MByte\\ \hline
          Runtime between exchanges QueenBee &21\,minutes\\ \hline   
          Number Replicas &16 \\ \hline
          Number of Replica Exchanges \jhanote{See caption for Q} &1000\\ \hline
          Total Runtime: &??   \\ \hline
	\end{tabular}
	\caption{REMD Application Characteristics\label{tab:app_stats}
          \jhanote{Is 1000 the the total number of exchanges or the number of exchanges that a single replica will undergo?}}
\end{table}

Further, the reliability capabilities of the proposed framework have
been extensively researched by inserting certain faults into the
systems. In our scenario, we killed selected replica processes and
measured the time required by Migol to restarting the system.  Due to
the selected monitoring intervall of one minute and failure threshold
of 2 tries, the failure detection time averages to 2.5\, minutes.

As shown in Figure~\ref{fig:performance_perf_submission} the recovery
time required for the restart of the job amounts to about 42
seconds. This is mainly caused by the complex interactions conducted
with the Migol backend (cmp. section~\ref{sec:migol}): The monitoring
service initializes the restart at the Migration Service, which again
verifies the state of the application. A major performance penalty is
the delegation-on-demand mechanism required to obtain the credential
of the user from the AIS. For this purpose the creation of a
public-private key pair is necessary. The Job Broker Service then
discovers and allocates resources.  The job start itself is conducted
via the Advance Reservation Service. In contrast to the GRAM, the ARS
is not bound to a single Grid site, i.\,e.\ if allowed by the
administrator it is able to conduct reservation respectively job
starts at any resource in the Grid. Local resource management systems
are detected during the runtime of the job. However, this feature is
associated with some overhead in contrast to a simple GRAM submission.
% \begin{itemize}
%     \item The  Migration Service is called - the services verifies the state of the application and tries to trigger a checkpoint
%     \item Job Broker Service: verifies state of resource job was allocated before. This requires some interaction with the MDS. In addition bandwidth information etc. are acquired
%     \item restarts the job at the respective resource. Due to architectural reasons Migol jobs are restarted through the Advance Reservation Service. It's most noted feature in contrast to the GRAM is the support for advance reservation. In contrast to the GRAM, the ARS is not bound to a single Grid site, i.\,e.\ if allowed by the administrator it is able to conduct reservation respectively job starts at any resource in the Grid. Local resource management systems are detected during the runtime of the job. However, this feature is associated with some overhead in contrast to a simple GRAM submission.
% \end{itemize}   

While these results show that SAGA/CPR in conjunction with Migol
incurs some overhead, we think that this is acceptable compared to the
benefits a fault tolerant, self-healing infrastructure offers. In
addition, it must be noted that further simple yet effective
optimizations are possible. \jhanote{an example or two would make this
  claim more substantiative}

%Some comments with respect to solved REMD problem:    
% about 17 MB data to transfer for a migration

% The REMDgManager was functionally tested on the LONI Grid. The following aspects are from interest:
% \begin{itemize}
%     \item \textbf{Speedup}: This test determines how much faster the Grid-enabled replica exchange algorithm is in comparison  to a sequential and/or a not Grid-enabled version.
%     
%     \item \textbf{Migol Overhead}:  During this test the overhead imposed by the Migol framework in comparison to a traditional SAGA/Globus infrastructure is evaluated. This overhead is mainly caused by the application-level monitoring, but can also be attributed to the necessary interaction with the Migol backend, e.\,g.\ for the registration of checkpoints. The following  metrics are evaluated:
%     \begin{itemize}
%         \item percentage overhead in runtime dependent on the monitoring interval
%         \item runtime overhead of Migol job submission and meta-data registration. 
%     \end{itemize}                                                           
%     
%     \item \textbf{Fault Injection Test:}  During this test, the functionality of the Migol framework is assed by introduction of different faults during the application run, e.\,g.:
%     \begin{itemize}
%         \item the failure of one or more worker nodes, or
%         \item the failure of the master node.        
%     \end{itemize}                       
%     During the test the correctness of the Migol framework and application is verified. In addition, the following metrics are determined:
%     \begin{itemize}
%         \item failure detection and recovery time
%         \item application runtime in correlation to the number of faults
%         \item recovery time versus false failure detection rate dependent on the monitoring intervall.
%     \end{itemize}
%                                                                                  
%     \item \textbf{Scalability test}: This test determines up to how many worker nodes the infrastructure is able to scale.
%     \item \textbf{Extended statistical analysis}: This evaluation determines the failure rates during REMD simulation runs. Further, it will be assessed how many of these real life failures can be detected and recovered by Migol. 
%                                                 
%     \alnote{The last two aspects should go due to timing reasons into the Phil Trans paper.}
% \end{itemize}
                                             
     
\Section{Related Work}

% TODO AM: add some SAGA-CPR background

\subsection*{Previous CPR Efforts for Distributed Environments}

Checkpointing and rollback recovery is widely used in Grids. For
example, the Condor/PGRADE system~\cite{DBLP:conf/eagc/KovacsK04}
consists of a checkpointing mechanism for PVM applications and uses
Condor-G~\cite{citeulike:291860} for scheduling.  While PGRADE
emphasises an integrated user-level checkpoint approach, we believe
that this approach is not suitable for a heterogeneous Grid
landscape. Further, the framework does not ensure the fault-tolerance
of the service infrastructure sufficiently.
                                 
Several framework for high throughput computing and task farming exist:
% Condor-G~\cite{citeulike:291860} allows the management of multi-site
% computations.
Nimrod-G~\cite{buyya00nimrodg} is a specialized framework for task
farming. The framework provides a user interface for describing task
farming applications. For remote execution of jobs Globus is
used. While the user interface is very suitable for parameter studies,
more demanding problems such as REMD simulations require a further
application-level integration.

The Legion~\cite{689541} Grid middleware provides a object-oriented
programing model, which allows the simple instantiation of distributed
tasks. While the provided abstraction is useful for developing Grid
applications, it relies on proprietary protocols and is not compatible
with OGSA-based service-oriented Grids.

Nimrod-G and Legion as also most Grid resource management systems,
such as Condor-G~\cite{citeulike:291860} and GridWay~\cite{Montero05},
provide basic fault tolerance support by automatic re-scheduling of
failed task. Advanced features such as the management of checkpoints
are not supported. Further, these frameworks rely on a very simple
failure detections mechanisms -- usually by simply polling the job
state at the Globus gatekeeper. This allows the detection of some
errors, but application-level failure detectors as used by the
Migol/SAGA library can detects much more complex errors. For example,
especially parallel applications can fail quite inconsistently: in the
best case the application aborts, at worst the application hangs
indefinitely. These kind of failures are not visible at Grid resource
management system level. Further, these frameworks or scheduler focus
on individual aspects, e.\,g.\ Nimrod-G focuses on task farming or
GridWay on meta-scheduling. Migol aims to provide an overall
autonomic, self-healing infrastructure, which addresses the fault
tolerance of Grid applications and the infrastructure itself.

At the level of related application programming interfaces for
checkpointing, proprietary interfaces are dominant. This is because
applications most often rely on application level checkpointing, and,
related, perform also their own checkpoint management (checkpointing
policies, frequencies, dependencies, staging etc).  The Open Grid
Forum's\footnote{\texttt{http://www.ogf.org}} GridCPR group (Grid
CheckPoint and Recovery) made an early attempt to describe a generic
CPR architecture, and to define a generic CPR API, which would support
applications to manage their complete checkpoint/recovery life
cycle~\cite{ogf_cpr_arch}.  Based on that architecture, and on a set
of CPR use cases~\cite{ogf_cpr_uc}, the SAGA group in OGF defined the
CPR API package~\cite{ogf_cpr_draft} (work in progress), whose
implementation is described in this paper.  The rendering of the CPR
API in the SAGA API framework allows (a) to seamlessly combine CPR
operations and other high level Grid programming abstractions provided
by SAGA, and (b) to abstract from the actual implementation of the CPR
mechanism.  The CPR API which has been demonstrated with the Migol
fault tolerance framework, can work just as well with, for example,
the XtreemOS system level checkpointing
capabilities~\cite{xtreemos_cpr}.


\subsection{Previous Attempts at RE simulations in Distributed
  Environments}
% \jhanote{We need to compare our approach with i) WISDOM and ii)
%   Folding@HOME}             

\alnote{Several project, such as Folding@home~\cite{folding} and
  Wisdom~\cite{wisdom}, rely on distributed infrastructures for
  computational biology and chemistry. While Folding@home utilizes a
  similar replica exchange algorithm~\cite{PhysRevLett.86.4983} it is
  based on BOINC~\cite{1033223}, a cycle scavenging infrastructure.}

%   \kimnote{Is this true? I know early attempts with Folding@home only
%     focused on parallel replica dynamics that does not need exchange!
%     They only get independent run results limited to the short time,
%     and use those large number of simulations for predicting kinetics,
%     I think}

\footnote{Folding@Home~\cite{PhysRevLett.86.4983} is parallel replica
  dynamics but that is a special case of replica exchange; when a
  certain event happens, there is a need for coordination amongst ALL
  replicas. We should maybe point this out, but I think it is fair at
  this level of detail, to consider folding@Home to be in the same
  application class to effectively parallelize simulations}

In contrast, general, multipurpose Grids as the TeraGrid, which
provide a dedicated runtime environment for applications with
different characteristic, e.\,g.\ loosely coupled, tightly coupled and
data-intensive applications, desktop Grids rely on volunteers. This
kind of environment is only suitable for loosely coupled
applications. Multipurpose Grids have several advantages: Issues as
trust and security are well researched. Further, it is possible to run
larger application chunks. The REMD manager splits an scenario into
multiple parts -- each part can be represented by a MPI application
utilizing up to 32 nodes. \jhanote{needs refinement will do}
\kimnote{Yes, this is MPI enables and there is no limitation on the
  number of nodes!}

The Wisdom project utilizes the EGEE infrastructure for molecular
docking simulations used e.\,g.\ for evaluation of drugs. While this
application has a similar high throughput characteristic, the project
is currently tied to the gLite~\cite{glite2008} Grid middleware.

In contrast to Wisdom and Folding@home, our approach is not restricted
to a specific Grid middleware -- by utilizing the SAGA standard the
REMD manager can easily utilize resources managed by different Grid
platforms. SAGA provides a well-defined abstraction, which is capable
of handling both scenarios. Using this approach the application can
easily used with different middleware platforms, Folding@home could
then be deployed on a BOINC infrastructure as well as on a
multipurpose Grid, such as TeraGrid.

\alnote{Should we add some details regarding the scientific results of
  WISDOM and Folding@home and how they differ from our REMD with NAMD?
  Or are we just comparing the Grid infrastructure?}  \jhanote{In
  response to immediateley preceeding alnote, IMHO we don't need to
  address scientific results, but will just say the ``size of the
  problem'' that can be studied is limited}



% In addition, SAGA provides many benefits for applications.  SAGA
% offers a feature rich programing abstractions, which can be used in
% conjunction with the fault tolerant Migol framework, but also with
% other Grid middleware platforms.
% 

%%------------------------------------------------------------------------------
\Section{Conclusion and Future Work}

We have developed a fault-tolerant framework that implements a
commonly occuring application usage pattern -- a loose-coupling of
multiple tightly-coupled applications -- whilst being general purpose
and extensible to different usage patterns, deployment scenarios and
specific simulation codes.

The fault-tolerant framework used to implement RE simulations in a
production environment is created using the distributed programming
interfaces provided by SAGA and its coupling to Migol to provide the
fault-tolerant features.  SAGA provides a middleware-independent,
programing abstraction for distributed environments. RE simulations
utilize the new SAGA-CPR API, which provides an ideal interface for
Grid applications to interface with a checkpoint-recovery
infrastructure, such as Migol. Using the newly developed SAGA adaptor
for Migol, any SAGA application can re-use Migol's fault tolerant
services for monitoring and recovery.  The application developer is
not required to provide any special code, solely the Migol adaptor
must be configured.  The framework has strong self-healing
capabilities: critical services, such as the Application Information
Service (AIS) are able to automatically detect failures and
reconfigure themselves, and thus addresses common failure modes in
distributed environments without user interactions.
%fault tolerance of Grid applications by handling common
%failures in Grid applications transparently without user interactions.
In case of failures, e.\,g.\ a node-crash, applications are
automatically restarted from the last saved
checkpoint. % Migol is designed with focus on fault tolerance.

In contrast to other replica-exchange implementations on distributed
simulations it is critical to note and emphasis the general usability
and extensibility of our approach -- across different infrastructure,
across a range of scientific applications and usage patterns (such as
the multiple variant of the replica-exchange).

\jhanote{In future work, we need to mention that we are deploying this
  infrastructure on a real distributed system (LONI) and are using it
  to study the binding interactions of peptide-RNA (Joohyun, would you
  agree?). We will report on the specific science results obtained
  using this approach in publication TBD but most likely Phil Trans of
  Royal Soc A}

\Section{Acknowledgements}
This work would not have been possible without the efforts and support
of the wider SAGA team, especially Hartmut Kaiser, Ole Weidner and
Joao Abecasis. We also thank Daniel Katz for hosting AL at the CCT,
when this work was conceived.  Important funding for SAGA
specification and development has been provided by the UK EPSRC grant
number GR/D0766171/1 (via OMII).  SJ acknowledges the e-Science
Institute, Edinburgh for supporting the theme, ``Distributed
Programming Abstractions''.  This work has also been made possible
thanks to the internal resources of the Center for Computation \&
Technology (CCT) at Louisiana State University and computer resources
provided by LONI.  \bibliographystyle{IEEEtran}
\bibliography{saga,literatur}
\end{document}


%TODO Future Work

%Migol provides an integrated solution for the management of data and
%compute resources.

% A main limitation of current Grid infrastructures is the restricted availability of 
% precise resource information. Managing this information uncertainty is difficult and often leads to
% a trial-and-error approach. For example, a transfer service cannot 
% differentiate between a harddisk failure and a transient network failure. While a retry 
% will resolve a transient failure, in case of a harddisk failure this approach will very likely not be successful.
% An adaptive strategy for tuning fault detection timeouts is therefore essential to achieve a 
% sufficient reliable fault detection while maintaining an acceptable performance.        

% Granularity of SAGA API not always well suited for Grid service interactions.
%%------------------------------------------------------------------------------     




% \begin{abstract}   {Grid Computing, Task Farming, SAGA, Migol}
% A major challenge in a dynamic Grid with thousands of machines connected to
% each other is fault tolerance. The more resources and components involved, the
% more complicated and error-prone becomes the system. 
% Migol~\cite{schnorLuckow08} is an adaptive Grid middleware,
% which addresses the fault tolerance of Grid applications and services 
% by providing the capability to recover applications from checkpoint files 
% transparently. 
% 
% SAGA~\cite{SAGA_Goodale06a} provides a standardized, application-level API for 
% common Grid scenarios, e.\,g.\
% the management of files and jobs. The new SAGA Checkpoint Recovery (CPR) package addresses checkpointing 
% and the automatic recovery of Grid applications. In this paper, we describe the design of 
% the SAGA CPR package, the integration of the CPR adaptor with the Migol infrastructure, 
% and our experiences with running a large scale SAGA CPR based task farming application 
% in a real Grid environment. 
% \end{abstract}   

%%------------------------------------------------------------------------------
% Open Topics:
% How long running are Map-Reduce sorting problems?
% Sorting with MapReduce: 1TB data => runtime 891 s (Dean, Google)
% SAGA provides a more low level primitives than Hadoop or Google MapReduce.
% What failure detection timeouts per task should be used?
% Performance measurement: without versus with failures    
% Comparison w/ Google MapReduce infrastructure:
%         - Master responsible for allocating mapper tasks (close to data)   
% Limitations of SAGA map-reduce
%       - low-level handling of file transfers and jobs required (ft aspect can be handled by Migol) 
%       - no sorting of data between map and reduce



% Long-running applications can significantly benefit from Migol:
% Without human interaction, recovery times are minimized, and the
% application and infrastructure utilization is enhanced.  While the
% Google's map-reduce infrastructure is tailored to the special needs
% of Google, SAGA provides an open standard, which allows the
% middleware independent implementation of MapReduce. Migol can
% provide a fault-tolerant run-time for map and reduction tasks
% handling resource allocation across VOs, staging, starting and
% re-starting of tasks transparently.
 
% Fault tolerance important:
% - One empty fail or the failure of a mapper task should not mess up the entire computation
% - if a particular input does not work - infrastructure will eventually give up
% - no reduce can start until map is complete - a single slow disk controller can rate-limte the whole process
% - master can re-execute slow-moving tasks redundantly (use results who first finishs)
% - mapreduce factors out synchronization
% - Reduce phase cannot start until all map tasks finished, i.\,e.\ it is not possible to achieve a result unless all 
% tasks have been finished.



% This paper is structured as follows: after an overview about related work 
% in section~\ref{sec:related} and the Migol service framework in 
% section~\ref{sec:migol}, this paper describes in detail the design of the 
% Checkpoint Recovery API for SAGA as well as the 
% the Migol adaptor. In section~\ref{sec:exp} we present our experiences 
% with checkpoint recovery of a task farming application in the 
% LONI~\cite{Allen:2003xy} Grid environment.      
   

%%------------------------------------------------------------------------------

% The objective of SAGA CPR is to mostly hide the complexity of a
% GridCPR infrastructure, such as making calls to different
% infrastructure services for registering information etc., from the
% application developer. Most interactions are handled transparently
% within the adaptor.  The SAGA CPR package provides a generic,
% middleware-independent API to a GridCPR middleware.
    
% For the management of checkpoint metadata the SAGA CPR API use the
% namespace paradigm to hierarchically organize files and directories.
% Checkpoint directories are used to group checkpoints. Parallel
% applications often write one checkpoint per processor, i.\,e.\ in
% case of $n$ processors a CPR checkpoint would consists of $n$ files.
% Thus, a CPR checkpoint is designed as container for multiple
% physical or logical files.
% 
%                                                       
% For execution and management of CPR jobs, SAGA defines a CPR job.
% The management of CPR jobs is similar to regular jobs: A job is
% defined by a job description. In contrast to normal jobs, CPR jobs
% require two job descriptions -- one for starting and another one for
% restarting the application.  Jobs are started using the job service.
% In addition to the normal job controls, a CPR job can be queried for
% checkpoint metadata, and it can be explicitly checkpointed or
% recovered.
% 
% In general, fault detection is done by a monitor process, which
% periodically sends keep-alive messages to applications.  Since the
% remote monitoring capability for applications can be completely
% hidden within the SAGA adaptor no explicit API is required. The SAGA
% adaptor can transparently start a monitoring endpoint without
% requiring any user interaction.  In case a SAGA application does not
% respond to a heartbeat message for a certain time a recovery is
% initiated by the Grid middleware, e.\,g.\ by Migol's MRS.

% Remote steering of SAGA applications can be implemented using the
% SAGA Monitorable API in conjunction with the job self object.

% With the described functionality the SAGA CPR package is well suited to 
% provide a clean abstraction to the Migol middleware. 
% Of course, the package can also be facilitated by another middleware adaptor. 
% In the following the Migol SAGA adaptor is described.   

% \subsection{SAGA CPR Migol Adaptor}
% 
% Currently, a C++ and Java implementation for SAGA exists. Although the Migol backend mainly consists 
% of web services Migol especially addresses the fault tolerance of long running e-Science applications written
% in C/C++. These  computing intensive applications have high performance requirements and, in general, require 
% native libraries, e.\,g.\ for numerically computations, or MPI for cluster communication. Thus, we focus on the 
% SAGA C++ reference implementation~\cite{Kaiser:2006qp}. 
% 
% % SAGA enables a seamless integration of an application into the Grid while Migol provides infrastructure 
% % services, such as monitoring, resource reservation and allocation. For example, an application 
% % can initiate a migration in case it detects a failure using the SAGA API. Migol will then be 
% % migrated e.\,g.\ it allows the application to start an migration in case it detects a 
% % failure or a performance bottleneck.  
% 
% \begin{figure}[t]
%   \centering
%   \includegraphics[width=0.6\textwidth]{saga-migol-layered}
%   \caption{SAGA Adaptor for Migol infrastructure}
%   \label{fig:saga-migol-layered}
% \end{figure}  


% \section{Experiences with a Large-Scale Task-Farming Application in the LONI Grid}
% \label{sec:exp}       
% 
%         
% %do we use checkpoints or do we automatically restart the task
% Figure~\ref{fig:saga-taskfarming} gives an overview about the task farming scenario. All tasks are
% distributed using a script and Migol's Job Broker Service.
% %  Each worker will compute a certain 
% % variation of the graph network. Depending on the graph size, the runtime of a single task  can 
% % be as high as \textbf{TODO}  days. 
% To enable the monitoring of a tasks, each task must initialize a \texttt{saga::session} object 
% before starting the computation.  Then, the Migol adaptor starts and registers the monitoring endpoint. 
% %TODO what is the task size/runtime of a single task
% The MRS can now monitor the application.  For error detection we use a timeout of 5 minutes.
% \begin{figure}[t]
%     \centering
%         \includegraphics[width=\textwidth]{saga-taskfarming}
%     \caption{Fault Tolerant Task Farming Scenario}
%     \label{fig:saga-taskfarming}
% \end{figure}
% 
% 
% 
% 
%   
% 
% During the runtime, we simulate the failure of a number of worker
% processes.  The Migol MRS successfully detects the respective faults
% and recovers the worker nodes correctly. Even with a high failure
% rate, we were able to obtain the results of our computation.
